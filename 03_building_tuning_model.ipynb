{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f913ea-baa9-4b11-81fc-aab2af393652",
   "metadata": {},
   "source": [
    "I based my initial selection for imports off the work we did in the NLP Practice breakfast hour challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95150329-11b5-4600-b86d-991db9701b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import functions as fun\n",
    "from lists import expanded_stopwords, expanded_proper_names\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e13433b-bd35-4a58-bac0-3d6b93af2509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>title</th>\n",
       "      <th>all_words</th>\n",
       "      <th>submission_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>submission_word_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>no_selftext</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1656261965</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>Which version of Klingons will appear in SNW?</td>\n",
       "      <td>Which version of Klingons will appear in SNW?</td>\n",
       "      <td>9.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1656254308</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>On the Gorn and language</td>\n",
       "      <td>On the Gorn and language</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656248567</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>What are some good things that can be said abo...</td>\n",
       "      <td>What are some good things that can be said abo...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1656238740</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>A Lord of the Rings reference in SNW 1x08</td>\n",
       "      <td>A Lord of the Rings reference in SNW 1x08</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1656238132</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>The sword props used in SNW 1x08 are replicas ...</td>\n",
       "      <td>The sword props used in SNW 1x08 are replicas ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc   selftext subreddit_name  \\\n",
       "0   1656261965  [removed]       startrek   \n",
       "1   1656254308  [removed]       startrek   \n",
       "2   1656248567  [removed]       startrek   \n",
       "3   1656238740  [removed]       startrek   \n",
       "4   1656238132  [removed]       startrek   \n",
       "\n",
       "                                               title  \\\n",
       "0      Which version of Klingons will appear in SNW?   \n",
       "1                           On the Gorn and language   \n",
       "2  What are some good things that can be said abo...   \n",
       "3          A Lord of the Rings reference in SNW 1x08   \n",
       "4  The sword props used in SNW 1x08 are replicas ...   \n",
       "\n",
       "                                           all_words  submission_length  \\\n",
       "0      Which version of Klingons will appear in SNW?                9.0   \n",
       "1                           On the Gorn and language                9.0   \n",
       "2  What are some good things that can be said abo...                9.0   \n",
       "3          A Lord of the Rings reference in SNW 1x08                9.0   \n",
       "4  The sword props used in SNW 1x08 are replicas ...                9.0   \n",
       "\n",
       "   title_length  submission_word_count  title_word_count  no_selftext  \\\n",
       "0          45.0                    1.0               8.0          0.0   \n",
       "1          24.0                    1.0               5.0          0.0   \n",
       "2          61.0                    1.0              13.0          0.0   \n",
       "3          41.0                    1.0               9.0          0.0   \n",
       "4         119.0                    1.0              22.0          0.0   \n",
       "\n",
       "   avg_word_length  \n",
       "0         4.625000  \n",
       "1         4.000000  \n",
       "2         3.692308  \n",
       "3         3.333333  \n",
       "4         4.090909  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df = pd.read_csv('data/cleaned_all_text2022-06-29.csv')\n",
    "\n",
    "whole_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f714a9a5-ae0e-41aa-9e65-3e64e41b4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_df[['subreddit_name', 'all_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96423d-bcd5-42bf-b7e1-266ff80f52f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Accuracy\n",
    "\n",
    "Our baseline accuracy is 50.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55197498-414c-49ce-a663-7450d7628006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startrek    0.509214\n",
       "starwars    0.490786\n",
       "Name: subreddit_name, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit_name.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3902cbf6-8ad4-4c19-99ec-d544e39ff1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test phrase: my computer computes computationally\n",
      "Test phrase lemmatized: my computer computes computationally\n",
      "Test phrase stemmed: my comput comput comput\n",
      "\n",
      "Test phrase: studies studying cries cry\n",
      "Test phrase lemmatized: study studying cry cry\n",
      "Test phrase stemmed: studi studi cri cri\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    split_text = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])\n",
    "\n",
    "def stem_text(text):\n",
    "    split_text = text.split()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_text])\n",
    "\n",
    "test_phrase1 = 'my computer computes computationally'\n",
    "print(f'Test phrase: {test_phrase1}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase1)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase1)}')\n",
    "print('')\n",
    "\n",
    "test_phrase2 = 'studies studying cries cry'\n",
    "print(f'Test phrase: {test_phrase2}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase2)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4027de-85f0-40c0-98e7-f4b5642eaaf3",
   "metadata": {},
   "source": [
    "# Prepping Data for Modeling\n",
    "\n",
    "Getting training and test data set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde5407a-1409-4f29-8a76-abcfb2bb5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209,)\n",
      "y_train shape: (5209,)\n",
      "X_test shape: (1737,)\n",
      "y_test shape: (1737,)\n",
      "\n",
      "Dataframe shape: (6946, 2)\n",
      "\n",
      "y_train value counts: startrek    0.509119\n",
      "starwars    0.490881\n",
      "Name: subreddit_name, dtype: float64\n",
      "y_test value counts: startrek    0.509499\n",
      "starwars    0.490501\n",
      "Name: subreddit_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['all_words']\n",
    "y = df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('')\n",
    "print('y_train value counts:', y_train.value_counts(normalize = True))\n",
    "print('y_test value counts:', y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe64ddd-8b8e-4fae-9e18-c89170177a71",
   "metadata": {},
   "source": [
    "# General Notes on Modeling\n",
    "## Previous Work\n",
    "I developed several models in which I didn't eliminate proper names, but as it seemed too easy, per Hank's suggestion I eliminated the proper names and franchise specific references that appeared in the top 150 words from the overall list and from each show. Even after using a list of names/terms from the two franchises, I felt it was a little too easy. For that reason, in this notebook, I worked from an expanded list of names/terms from the shows that I eliminated from the models in the form of stopwords.\n",
    "\n",
    "I saved the previous modeling I did without removing show specific names/words and with the shorter list. I didn't include them here in order to avoid clutter. Please let me know if you want to see them.\n",
    "\n",
    "## Notes On These Models\n",
    "As I developed the models in this notebook, I started from the best parameters from that first modeling process as my starting point with these models.\n",
    "\n",
    "In the course of tuning the models in this notebook, I used 'expanded_proper_names' and 'expanded_stopwords'. The first is the longest list of proper names and show specific references I developed. The other is that list combined with the scikit-learn 'english' stopwords. This enabled me to see if the 'english' stopwords helped the models or not.\n",
    "\n",
    "In the models without the expanded stopwords, I checked lemmatized and stemmed word lists, but as a rule found that the stop words were more helpful. Between that and the way that I'm removing the stop words, which doesn't play nice with lemmatizing and stemming, I'm leaving those out in these models.\n",
    "\n",
    "## Observations\n",
    "### On Parameters\n",
    "In previous modeling with no or fewer franchise-specific stopwords, I noticed that the best models often had best 'ngram_range' = (1,2) and the 'english' stopwords taken out. Interestingly, and perhaps intuitively, with more of the franchise specific words eliminated, the best 'ngram_range' is (1,1) and the models are tending to perform better with the 'english' stopwords *not* removed from the vectors more frequently than I would have anticipated. Eliminating enough franchise-specific words would logically turn many two word ngrams into gibberish, and it's not illogical that stopwords would become more helpful to modeling in the absence of these other words.\n",
    "\n",
    "### On Tuning Decisions\n",
    "GridSearchCV prefers the best parameters on the training data it's being tuned with. I watched the test score to make parameter decisions, so in some cases I chose parameters that GridSearchCV didn't select as optimal for that reason.\n",
    "\n",
    "## Best Models\n",
    "### Model 8, TfidfVectorizer, MultinomialNB, with 'no_selftext'\n",
    "This was the best model as measured by score, with particular focus on test score. It's also the best fit model. I experimented with adding other features, but the 'no_selftext' feature was most predictive (in fact, other features made the model weaker).\n",
    "\n",
    "The parameters:\n",
    "* 'nb__alpha': 0.2 \n",
    "* 'tvec__max_df': 0.7 \n",
    "* 'tvec__max_features': 4800 \n",
    "* 'tvec__ngram_range': (1, 1) \n",
    "* 'tvec__stop_words': expanded_proper_names\n",
    "* 'no_selftext' added to dataframe\n",
    "\n",
    "Training score: 0.9249376079861777\\\n",
    "Test score: 0.8733448474381117\n",
    "\n",
    "### Model 9, TfidfVectorizer, LogisticRegression, with 'no_selftext'\n",
    "This was the best LogisticRegrssion model as measured by score, with particular focus on test score. I'm particularly interested in LogisticRegressions so that I can do some inference and see what I learn. Again, I experimented with adding other features, but the 'no_selftext' feature was most predictive (in fact, other features made the model weaker).\n",
    "\n",
    "The parameters:\n",
    "* 'log__C': 1.4\n",
    "* 'tvec__max_df': 0.12\n",
    "* 'tvec__max_features': 6500\n",
    "* 'tvec__ngram_range': (1, 2)\n",
    "* 'tvec__stop_words': expanded_stopwords\n",
    "* 'no_selftext' added to dataframe\n",
    "\n",
    "Training score: 0.9149548857746208\n",
    "Test score: 0.8480138169257341\n",
    "\n",
    "## Acknowledgements\n",
    "I modeled my work flow initially on the NLP Breakfast Hour, though I think it wound up changing a lot from there.\n",
    "\n",
    "**IMPORTANT: I ran the following models with n_jobs = -1. If you want to run any of them without n_jobs = -1, you can change `'fun.pip_grid_njobs'` to `'fun.pip_grid'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a594c52-412c-4785-9f7c-2301de306222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 1, TfidfVectorizer, MultinomialNB\n",
    "My best TfidfVectorizer, MultinomialNB model was my best model with proper names, so I started with this model here.\n",
    "\n",
    "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.7, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': expanded_proper_names}\n",
    "Training score: 0.9199462468803993\n",
    "Test score: 0.8514680483592401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8d7212-36a5-4823-94b2-4d4417c7796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 37.219505071640015\n",
      "Best parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.4, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9187943943175274\n",
      "Test score: 0.8497409326424871\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "729b027a-415f-4d95-b7fa-fe434fab7749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 39.72764801979065\n",
      "Best parameters: {'nb__alpha': 0.3, 'tvec__max_df': 0.9, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9207141485889806\n",
      "Test score: 0.8508923431203224\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.3, 0.5, 0.7]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "318f6fa3-44d5-4c7e-890a-ef0170710c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 38.770002126693726\n",
      "Best parameters: {'nb__alpha': 0.3, 'tvec__max_df': 0.9, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9207141485889806\n",
      "Test score: 0.8508923431203224\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.2, 0.3, 0.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f99db18-3f2a-470b-8038-089c7e7a5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 40.09021806716919\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.9, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9218660011518526\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.25, 0.3, 0.35]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5cfe131-f91f-4386-bbf8-4fc591d8a51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 12.288998126983643\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.9, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9218660011518526\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_700, 4_900, 5_100],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e2964d6-0f5c-4f89-bd36-b4573dc56f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 12.847345113754272\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.9, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a705d43-c314-4603-8303-114086cb6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 8.708423852920532\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.9, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_700, 4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce0c565-32aa-4874-858b-8f19aea14ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 4.433788061141968\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.8, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.8, 0.9, 1],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2910b034-63d1-48d7-857f-6efc5d3d56c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 3.195868730545044\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.7, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.7, 0.8],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "270ba9f5-4f17-4a7e-a6a6-7c67ac513e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 4.024868011474609\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.7, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.7],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c003077-ca18-4839-ad27-b46e2b89debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 0.5145351886749268\n",
      "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.7, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.9199462468803993\n",
      "Test score: 0.8514680483592401\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_proper_names],\n",
    "    'tvec__max_df': [0.7],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0657d-bb3c-4833-8489-971285b7bd95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 2, CountVectorizer, MultinomialNB \n",
    "Without proper names removed, the second best model was the optimized CountVectorizer/MultinomialNB model, so I worked through this next.\n",
    "\n",
    "Time to run 0.4991631507873535\n",
    "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_proper_names, 'nb__alpha': 0.5}\n",
    "Training score: 0.8871184488385486\n",
    "Test score: 0.8457109959700633\n",
    "\n",
    "This is the better fit of the two top performing models (performance being measured by Test score (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9744268b-7353-482f-b072-0a862db87555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 35.862080097198486\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fc1bcc0-9242-44eb-b6ec-451eae6ebed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 35.79885125160217\n",
      "Best parameters: {'cvec__max_df': 0.3, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.887310424265694\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.2, 0.3, 0.4],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bd85b2b-4439-4fbe-a495-6273fbb17344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 12.410980939865112\n",
      "Best parameters: {'cvec__max_df': 0.4, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8865425225571127\n",
      "Test score: 0.844559585492228\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.4],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d9904a0-ba71-47d3-be7d-835fc7be75b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 25.22023916244507\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.35],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25fd3422-d34b-4b9c-9157-c867bc4b4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 28.40887975692749\n",
      "Best parameters: {'cvec__max_df': 0.33, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8884622768285659\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.33, 0.34],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4306e219-814c-46db-b984-1d26919888cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 13.449173927307129\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68146a48-8936-4fbd-85ca-f779787c2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 12.116826295852661\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23029b30-879a-4fc5-9243-5ce265281e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 12.709076166152954\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_400, 6_500, 6_600],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e823130a-8d10-47ee-bf5c-00f06faa2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 13.818279027938843\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6800, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8880783259742753\n",
      "Test score: 0.844559585492228\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600, 6_700, 6_800],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3229bf3-945d-4e07-93b0-8b6456ec9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 9.655218839645386\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600, 6_700],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f0b711d-3f33-48d1-b121-23a7344db0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 4.406274080276489\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.25}\n",
      "Training score: 0.8905740065271646\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.25, 0.5, 0.75]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc86ccf6-e19e-45c7-95a0-ad362f84f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 4.289211988449097\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.6}\n",
      "Training score: 0.8861585717028221\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.4, 0.5, 0.6]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57091b9a-a795-45b7-bf46-da348e21f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 0.4991631507873535\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "#BEST PERFORMANCE, MODEL 2\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a325f0-4dee-46e8-9195-2e7399e534c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 3, CountVectorizer & LogisticRegression\n",
    "Without proper names removed, the optimized CountVectorizer and LogisticRegression combination was the third best, so I worked with that 3rd.\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stopwords, 'log__C': 1.1}\n",
    "Training score: 0.9537339220579766\n",
    "Test score: 0.8215313759355211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06bbf1a4-07de-43f3-85ad-2e75dc600907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.88155698776245\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.5}\n",
      "Training score: 0.9489345363793434\n",
      "Test score: 0.8151986183074266\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a72a90d4-c6fe-47b3-82be-a758f681f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.472047805786133\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9424073718564023\n",
      "Test score: 0.8105929763960852\n"
     ]
    }
   ],
   "source": [
    "# since the model was overfit, I'm going to try making C smaller\n",
    "\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7dbaa34-9d05-4c0a-99fd-9bef369f0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.166616916656494\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 5500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9479746592436168\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [4_500, 5_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa8d5c13-2bff-4041-98dd-b80c8ddea594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.362706899642944\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92739488-6418-484a-8246-a218a402bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.755382776260376\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1b04ddb0-5135-42ef-8709-0aa53156b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.713257312774658\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_250, 6_500, 6_750],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0db07669-e7c5-4652-9438-205cd8c7131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.442172288894653\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_400, 6_500, 6_600],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7ba5baa-db3b-4e38-9c43-9c55bea77cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.92202615737915\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 0.5}\n",
      "Training score: 0.93511230562488\n",
      "Test score: 0.8146229130685089\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09, 0.19, 0.29],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b39a1daf-992a-4c24-bd23-344c71d08825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.339927673339844\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 0.5}\n",
      "Training score: 0.93511230562488\n",
      "Test score: 0.8146229130685089\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.04, 0.09, 0.14],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b84bf5a5-560e-479e-9bb1-a9067bd91365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.070679903030396\n",
      "Best parameters: {'cvec__max_df': 0.08, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.0}\n",
      "Training score: 0.9564215780380111\n",
      "Test score: 0.8134715025906736\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.08, 0.09, 0.10],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6e7a237-350f-466b-9c0d-54000a5ccb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.323198318481445\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.9, 1.0, 1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5812d6b6-6d05-40d3-bff8-274d76a962f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.89455509185791\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [1.1, 1.2]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f4cd2a7-7292-49e1-8990-9c224c97b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6170868873596191\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "#BEST MODEL 3\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cf714-9204-46fa-898b-98cee90648ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 4, TfidfVectorizer, LogisticRegression\n",
    "Of the 4 combinations I tried without proper names/show references removed, this was the least effect, so I tried it last.\n",
    "\n",
    "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stopwords}\n",
    "Training score: 0.9383758878863505\n",
    "Test score: 0.8307426597582038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d951a3ee-6341-4d4b-8ec0-a06f487f5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 39.4936580657959\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.22, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9306968708005375\n",
      "Test score: 0.8290155440414507\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.02, 0.22, 0.42],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfaa34ff-d046-4b34-ba26-fc8bc2931790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 40.74934983253479\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12, 0.22, 0.32],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9805426c-c43c-40bc-8857-67f657340c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 38.62906289100647\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.07, 0.12, 0.17],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "986395d0-b460-451a-90ac-c84f4bbc9361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 40.75169014930725\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.10, 0.12, 0.14],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ded6db72-423b-4ee3-9dd5-3c841cd7cd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 42.18435192108154\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.11, 0.12, 0.13],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5e5311c-0020-4eca-85ca-e7b173bdddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 13.28843092918396\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [3_400, 4_900, 6_400],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9e76603-3ccc-4894-bff2-e82c92ea8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 13.605923175811768\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [5_400, 6_400, 7_400],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39856741-1d4e-4fb5-8e3f-07dc92e85272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 13.74911093711853\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [5_900, 6_400, 6_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f21e02a-ad5d-4dae-96bc-e5af1bc4c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 14.678189992904663\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9391437895949318\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_300, 6_400, 6_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c416fc73-d606-40e4-975c-a921450ed5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 9.958917140960693\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9391437895949318\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500, 6_600],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5483b8a6-bba3-4ffa-be3a-c4580f282929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 5.618391036987305\n",
      "Best parameters: {'log__C': 1.75, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.943367248992129\n",
      "Test score: 0.8295912492803684\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.25, 1.5, 1.75]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db1d5e4b-18f0-4da2-8e2a-84b18ec5437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 2.1659951210021973\n",
      "Best parameters: {'log__C': 1.0, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9293530428105202\n",
      "Test score: 0.8290155440414507\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78e7a670-d549-4396-bad6-86e86f1d0da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 2.173959970474243\n",
      "Best parameters: {'log__C': 1.25, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9360721827606067\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d147c19-5508-4783-9896-2edb85c82408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 2.3524134159088135\n",
      "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9383758878863505\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2eadd2c-7783-41ed-8b65-df0b903c950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 1.0933971405029297\n",
      "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.9383758878863505\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION OF MODEL 4\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249818d-871b-4b44-bf80-16e2330f1bc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 5, CountVectorizer, RandomForest\n",
    "\n",
    "Because the performance of this model is so much weaker than above, I didn't spend much time trying to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0347c443-d8b4-4372-8c9a-72ce829afef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 618.7906250953674\n",
      "Best parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 2500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'rf__max_depth': 5, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 5, 'rf__n_estimators': 150}\n",
      "Training score: 0.7387214436552121\n",
      "Test score: 0.7363270005757052\n"
     ]
    }
   ],
   "source": [
    "#THIS MODEL IS EXTREMELY WELL FIT BUT IT'S NOT NEARLY AS ACCURATE AS THE ONES ABOVE.\n",
    "\n",
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [50, 100, 150],\n",
    "    'rf__max_depth': [3, 5],\n",
    "    'rf__min_samples_split': [3, 5],\n",
    "    'rf__min_samples_leaf': [3, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31fb89d4-d5e3-4a28-abc6-888a39cf9bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 66.63195705413818\n",
      "Best parameters: {'cvec__max_df': 0.1, 'cvec__max_features': 2250, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'rf__max_depth': 7, 'rf__min_samples_leaf': 5, 'rf__min_samples_split': 3, 'rf__n_estimators': 200}\n",
      "Training score: 0.7224035323478595\n",
      "Test score: 0.7271157167530224\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'cvec__max_features': [2_250, 2_500],\n",
    "    'rf__n_estimators': [200],\n",
    "    'rf__max_depth': [7],\n",
    "    'rf__min_samples_split': [3, 5],\n",
    "    'rf__min_samples_leaf': [3, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd7050-01af-462c-9c8c-c0538efdb890",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 6, TfidfVecorizer, RandomForestClassifier()\n",
    "\n",
    "Again, this model is so much weaker than above, with no obvious factors to adjust to dramatically improve it, so I stopped tuning it pretty quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "344f0c66-2918-4021-88ae-2eebb344c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 16.51498818397522\n",
      "Best parameters: {'rf__max_depth': 3, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100, 'tvec__max_df': 1.0, 'tvec__max_features': 1000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.7066615473219428\n",
      "Test score: 0.6655152561888313\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'tvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'tvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [100], #50, 150\n",
    "    'rf__max_depth': [3], #5, 7\n",
    "    'rf__min_samples_split': [3], #, 5, 7\n",
    "    'rf__min_samples_leaf': [3] #, 5, 7\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbb1c863-5c99-47a7-84be-bb12be9ddd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 84.97272491455078\n",
      "Best parameters: {'rf__max_depth': 5, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100, 'tvec__max_df': 1.0, 'tvec__max_features': 2500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'})}\n",
      "Training score: 0.7066615473219428\n",
      "Test score: 0.6931491076568796\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'tvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'tvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [100, 150], #50, 150\n",
    "    'rf__max_depth': [5, 7],\n",
    "    'rf__min_samples_split': [3], #, 5, 7\n",
    "    'rf__min_samples_leaf': [3] #, 5, 7\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98407f-eb23-4b8a-a8ab-e03723cec443",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 7 VotingClassifier with AdaBoosting and GradientBoosting\n",
    "\n",
    "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': expanded_stopwords, 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
    "Training Score: 0.8957573430600884\n",
    "Test Score: 0.7910189982728842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3b7616fc-13b5-4872-b371-50cc38013c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.22982907295227\n",
      "Best Parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 2000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'vote__ada__n_estimators': 75, 'vote__gb__n_estimators': 75, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8107122288347092\n",
      "Test Score: 0.7328727691421992\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [50, 75],\n",
    "    'vote__gb__n_estimators': [50, 75]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aea66b7f-c64b-415a-b3d1-07ff9b7070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.06975388526917\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'vote__ada__n_estimators': 100, 'vote__gb__n_estimators': 100, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8174313687847955\n",
      "Test Score: 0.7501439263097294\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.9, 1.0],\n",
    "    'cvec__max_features': [2_000, 3_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [75, 100],\n",
    "    'vote__gb__n_estimators': [75, 100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "26097cf8-88f1-4c92-b687-81384cc3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.07606506347656\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 150, 'vote__gb__n_estimators': 150}\n",
      "Training Score: 0.82299865617201\n",
      "Test Score: 0.7512953367875648\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, 1.0],\n",
    "    'cvec__max_features': [3_000, 4_000],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [100, 150],\n",
    "    'vote__gb__n_estimators': [100, 150]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f02b914a-1337-4e90-90c7-d3b2ce596380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.22899413108826\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 200, 'vote__gb__n_estimators': 200}\n",
      "Training Score: 0.838932616625072\n",
      "Test Score: 0.7725964306275187\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, .96],\n",
    "    'cvec__max_features': [3_000, 3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [150, 200],\n",
    "    'vote__gb__n_estimators': [150, 200]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "952465eb-1fff-4249-8d3f-f62726dcb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.841930389404297\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 500, 'vote__gb__n_estimators': 350}\n",
      "Training Score: 0.9082357458245345\n",
      "Test Score: 0.7973517559009787\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [400, 500],\n",
    "    'vote__gb__n_estimators': [300, 350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "44d99ca1-8686-4ddc-9df1-d452d7dbbf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.3620388507843\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 600, 'vote__gb__n_estimators': 350}\n",
      "Training Score: 0.9243616817047418\n",
      "Test Score: 0.7956246401842256\n"
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [500, 600],\n",
    "    'vote__gb__n_estimators': [350, 400]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "581334ac-c371-48db-a5ad-ba5a40aa9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.751996278762817\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': 15}\n",
      "Training Score: 0.8352850835093109\n",
      "Test Score: 0.7795048934945308\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    'vote__tree__max_depth': [5, 10, 15],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6529595d-8b24-4cba-951f-30ba38809b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.699783325195312\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8957573430600884\n",
      "Test Score: 0.7910189982728842\n"
     ]
    }
   ],
   "source": [
    "#best version of model 7, with cvec__max_features: [3500]\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_000, 3_500, 4_000],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3590afb-0a8c-4531-b7ee-8f735dc599de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.455538749694824\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3400, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereafter', 'empty', 'show', 'whole', 'this', 'wherein', 'nine', 'him', 'what', 'their', 'me', 'cannot', 'eg', 'alone', 'mostly', 'whether', 'vader', 'other', 'themselves', 'become', 'namely', 'against', 'much', 'fett', 'least', 'data', 'former', 'myself', 'i', 'than', 'be', 'down', 'rebels', 'the', 'sith', 'done', 'nevertheless', 'besides', 'to', 'above', 'get', 'however', 'captain', 'amount', 'lightsaber', 'anyway', 'ourselves', 'first', 'is', 'out', 'hasnt', 'those', 'from', 'along', 'not', 'if', 'will', 'at', 'whose', 'picard', 'one', 'beforehand', 'side', 'further', 'twelve', 'have', 'am', 'was', 'us', 'mill', 'already', 'a', 'its', 'she', 'klingons', 'there', 'always', 'could', 'our', 'by', 'yourself', 'too', 'without', 'though', 'eleven', 'being', 'up', 'force', 'under', 'worf', 'else', 'becomes', 'each', 'itself', 'riker', 'himself', 'back', 'why', 'hereupon', 'forty', 'anyone', 'until', 'wherever', 'upon', 'pike', 'seems', 'call', 'boba', 'whither', 'of', 'take', 'here', 'six', 'etc', 'either', 'inquisitor', 'seeming', 'whence', 'whom', 'padme', 'would', 'nor', 'when', 'made', 'seemed', 'otherwise', 'before', 'contact', 'between', 'very', 'been', 'sixty', 'indeed', 'an', 'may', 'often', 'keep', 'how', 'anyhow', 'describe', 'thence', 'afterwards', 'wan', 'but', 'un', 'some', 'somewhere', 'strange', 'more', 'fifteen', 'elsewhere', 'order', 'fill', 'them', 'yourselves', 'must', 'that', 'he', 'almost', 'such', 'are', 'bill', 'starfleet', 'behind', 'wars', 'whenever', 'whereby', 'co', 'once', 'kirk', 'nobody', 'reva', 'throughout', 'over', 'perhaps', 'thick', 'whereas', 'own', 'find', 'ours', 'both', 'top', 'these', 'hence', 'third', 'yoda', 'another', 'beside', 'star', 'snw', 'ltd', 'anything', 'two', 'many', 'amoungst', 'yours', 'hers', 'should', 'so', 'discovery', 'twenty', 'sincere', 'thru', 'sometime', 'interest', 'couldnt', 'serious', 'detail', 'herself', 'it', 'part', 'where', 'full', 'well', 'even', 'nothing', 'do', 'any', 'move', 'same', 'thus', 'onto', 'we', 'moreover', 'mine', 'luke', 'neither', 'seem', 'hundred', 'paramount', 'front', 'ten', 'de', 'hereafter', 'your', 'into', 'no', 'seven', 'for', 'cant', 'they', 'ds9', 'meanwhile', 'off', 'about', 'voyager', 'name', 'as', 'trilogy', 'across', 'except', 'eight', 'five', 'with', 'whatever', 'something', 'ever', 'see', 'bottom', 'enterprise', 'skywalker', 'due', 'noone', 'palpatine', 'thin', 'whoever', 'can', 'anywhere', 're', 'only', 'sometimes', 'nowhere', 'per', 'herein', 'system', 'klingon', 'fifty', 'toward', 'now', 'towards', 'spock', 'federation', 'hereby', 'disney', 'con', 'most', 'fire', 'formerly', 'in', 'please', 'through', 'cry', 'everywhere', 'still', 'tos', 'became', 'whereafter', 'trek', 'while', 'which', 'maul', 'amongst', 'ie', 'somehow', 'his', 'among', 'were', 'during', 'then', 'beyond', 'within', 'others', 'found', 'might', 'or', 'several', 'all', 'few', 'go', 'had', 'give', 'together', 'last', 'who', 'never', 'via', 'her', 'empire', 'every', 'thereupon', 'three', 'kenobi', 'latter', 'thereby', 'darth', 'less', 'therein', 'therefore', 'anakin', 'none', 'because', 'everything', 'someone', 'you', 'everyone', 'st', 'becoming', 'jurati', 'after', 'since', 'has', 'rather', 'next', 'my', 'on', 'put', 'republic', 'below', 'although', 'inc', 'and', 'also', 'clone', 'again', 'jedi', 'four', 'mandalorian', 'latterly', 'tng', 'yet', 'enough', 'leia', 'whereupon', 'around', 'warp', 'borg', 'obi'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8965252447686696\n",
      "Test Score: 0.7869890616004606\n"
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_400, 3_500, 3_600],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cecea-6a2c-45bc-a7f5-9b64ae296f45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 8, TfidfVectorizer, MultinomialNB, Title and Submission (body) Lengths and Wordcounts\n",
    "Using best TfidfVectorizer parameters from above\\:\n",
    "'nb__alpha': 0.25\\ \n",
    "'tvec__max_df': 0.7\\ \n",
    "'tvec__max_features': 4800\\ \n",
    "'tvec__ngram_range': (1, 1)\\ \n",
    "'tvec__stop_words': expanded_proper_names\\\n",
    "Training score: 0.9199462468803993\\\n",
    "Test score: 0.8514680483592401\n",
    "\n",
    "Best Result with just 'no_selftext' added, alpha = .2:\\\n",
    "Training score: 0.9249376079861777\\\n",
    "Test score: 0.8733448474381117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "16aa3696-1c7d-472a-a05e-1ccb8fd03609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4805)\n",
      "vectorized_test concatenated shape: (1737, 4805)\n",
      "vectorized_train corrected index shape: (5209, 4804)\n",
      "vectorized_test corrected index shape: (1737, 4804)\n",
      "Training score: 0.5371472451526205\n",
      "Test score: 0.5371329879101899\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'submission_length', 'title_length', 'submission_word_count', 'title_word_count']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'submission_length', 'title_length', 'submission_word_count', 'title_word_count']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.25)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ff47f1a1-6701-4b28-a6c6-6b3eddaf6887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9235937799961605\n",
      "Test score: 0.8716177317213587\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.25)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8a7501ac-1519-4c96-995b-8e9bc80a9cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9170666154732194\n",
      "Test score: 0.8658606793321819\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.5)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b627101a-0d13-4871-b6e5-3197b2399791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9220579765789979\n",
      "Test score: 0.869314910765688\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.3)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a570724c-c538-4db0-b348-5001999b5092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9249376079861777\n",
      "Test score: 0.8733448474381117\n"
     ]
    }
   ],
   "source": [
    "# BEST VERSION\n",
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.2)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4e193965-555c-4cd7-ad50-9e0f9a46ca0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9272413131119217\n",
      "Test score: 0.8733448474381117\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.15)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2b3e185e-9d0f-486a-8e7b-14c0dc164402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9301209445191015\n",
      "Test score: 0.872769142199194\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b64769-a426-4d00-8504-acb3788d0cb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 9, TfidfVectorizer, LogisticRegression, Adding Features\n",
    "Using best TfidfVectorizer parameters from above\n",
    "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stopwords} Training score: 0.9383758878863505 Test score: 0.8307426597582038\n",
    "\n",
    "Best result occurs with just adding 'no_selftext':\n",
    "Training score: 0.9149548857746208\n",
    "Test score: 0.8480138169257341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "aea82568-b476-477f-a723-0cfaefc64145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6505)\n",
      "vectorized_test concatenated shape: (1737, 6505)\n",
      "vectorized_train corrected index shape: (5209, 6504)\n",
      "vectorized_test corrected index shape: (1737, 6504)\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'submission_length', 'title_length', 'submission_word_count', 'title_word_count']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'submission_length', 'title_length', 'submission_word_count', 'title_word_count']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "559944d4-ad7a-4870-b79b-d6b49a661823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best paramaters: {'log__C': 1.5}\n",
      "Training score: 0.9251295834133231\n",
      "Test score: 0.82671272308578\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('log', LogisticRegression(max_iter = 10_000))])\n",
    "\n",
    "pipe_params = {\n",
    "    'log__C': [0.5, 1, 1.5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid = pipe_params,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(vectorized_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print('Best paramaters:', gs.best_params_)\n",
    "print('Training score:', gs.score(vectorized_train, y_train))\n",
    "print('Test score:', gs.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1d488ab4-dbe2-4de7-a91a-68cd6213f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6502)\n",
      "vectorized_test concatenated shape: (1737, 6502)\n",
      "vectorized_train corrected index shape: (5209, 6501)\n",
      "vectorized_test corrected index shape: (1737, 6501)\n",
      "Training score: 0.9149548857746208\n",
      "Test score: 0.8480138169257341\n"
     ]
    }
   ],
   "source": [
    "# BEST RESULT\n",
    "\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f69dc48f-3cd4-450a-9d67-b3a43a74a3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6503)\n",
      "vectorized_test concatenated shape: (1737, 6503)\n",
      "vectorized_train corrected index shape: (5209, 6502)\n",
      "vectorized_test corrected index shape: (1737, 6502)\n",
      "Training score: 0.9130351315031676\n",
      "Test score: 0.8474381116868164\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext', 'avg_word_length']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext', 'avg_word_length']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "97c612a7-0e5d-440f-af69-fe692f441805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6503)\n",
      "vectorized_test concatenated shape: (1737, 6503)\n",
      "vectorized_train corrected index shape: (5209, 6502)\n",
      "vectorized_test corrected index shape: (1737, 6502)\n",
      "Training score: 0.8892301785371473\n",
      "Test score: 0.8411053540587219\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext', 'submission_length']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext', 'submission_length']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16a16d-ddc8-48df-932c-f794f58773e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
