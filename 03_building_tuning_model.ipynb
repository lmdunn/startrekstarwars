{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f913ea-baa9-4b11-81fc-aab2af393652",
   "metadata": {},
   "source": [
    "I based my initial selection for imports off the work we did in the NLP Practice breakfast hour challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95150329-11b5-4600-b86d-991db9701b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from functions import pipe_grid, pipe_grid_njobs, lemmatize_text, stem_text\n",
    "#import functions as fun\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e13433b-bd35-4a58-bac0-3d6b93af2509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>startrek</td>\n",
       "      <td>I’m beaming and I had to share - Sir Patrick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>startrek</td>\n",
       "      <td>America and the Star Trek Universe. Roe Vs Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startrek</td>\n",
       "      <td>Analysis: Star Trek: The Next Generation’ Gue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>startrek</td>\n",
       "      <td>One of the first occasions in which the word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>startrek</td>\n",
       "      <td>Is A TOS Reboot Coming Soon?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_name                                          all_words\n",
       "0       startrek   I’m beaming and I had to share - Sir Patrick ...\n",
       "1       startrek   America and the Star Trek Universe. Roe Vs Wa...\n",
       "2       startrek   Analysis: Star Trek: The Next Generation’ Gue...\n",
       "3       startrek   One of the first occasions in which the word ...\n",
       "4       startrek                       Is A TOS Reboot Coming Soon?"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cleaned_all_text2022-06-27.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96423d-bcd5-42bf-b7e1-266ff80f52f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Accuracy\n",
    "\n",
    "Our baseline accuracy is 57.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55197498-414c-49ce-a663-7450d7628006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "starwars    0.572228\n",
       "startrek    0.427772\n",
       "Name: subreddit_name, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit_name.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e2e74-2906-4836-8863-75b083bfa74c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building Functions to Lemmatize and Stem Text\n",
    "\n",
    "I'm using the workflow from the NLP Practice breakfast hour as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3902cbf6-8ad4-4c19-99ec-d544e39ff1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test phrase: my computer computes computationally\n",
      "Test phrase lemmatized: my computer computes computationally\n",
      "Test phrase stemmed: my comput comput comput\n",
      "\n",
      "Test phrase: studies studying cries cry\n",
      "Test phrase lemmatized: study studying cry cry\n",
      "Test phrase stemmed: studi studi cri cri\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    split_text = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])\n",
    "\n",
    "def stem_text(text):\n",
    "    split_text = text.split()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_text])\n",
    "\n",
    "test_phrase1 = 'my computer computes computationally'\n",
    "print(f'Test phrase: {test_phrase1}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase1)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase1)}')\n",
    "print('')\n",
    "\n",
    "test_phrase2 = 'studies studying cries cry'\n",
    "print(f'Test phrase: {test_phrase2}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase2)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4027de-85f0-40c0-98e7-f4b5642eaaf3",
   "metadata": {},
   "source": [
    "# Prepping Data for Modeling\n",
    "\n",
    "Getting training and test data set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde5407a-1409-4f29-8a76-abcfb2bb5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3753,)\n",
      "y_train shape: (3753,)\n",
      "X_test shape: (1252,)\n",
      "y_test shape: (1252,)\n",
      "\n",
      "Dataframe shape: (5005, 2)\n",
      "\n",
      "y_train value counts: starwars    0.572342\n",
      "startrek    0.427658\n",
      "Name: subreddit_name, dtype: float64\n",
      "y_test value counts: starwars    0.571885\n",
      "startrek    0.428115\n",
      "Name: subreddit_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['all_words']\n",
    "y = df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('')\n",
    "print('y_train value counts:', y_train.value_counts(normalize = True))\n",
    "print('y_test value counts:', y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed5a97-26e7-4b49-a17c-94d8bdab42e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling\n",
    "\n",
    "I built a function to streamline the pipeline/gridsearch writing.\\\n",
    "I modeled with out adding the proper name stop words first.\n",
    "\n",
    "I began by working on CountVectorizer and LogisticRegression, tweaking the parameters and running the model repeatedly. The score was very high to start, probably because of the proper names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bc657-a8f5-4ceb-b664-61c734da129b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building a Gridsearch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996402a1-58d3-4005-bb23-beeec29dae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_grid(pipe_params, grid_params):\n",
    "    '''\n",
    "    This function is designed to streamline gridsearching.\n",
    "    It returns a gridsearch named 'gs'\n",
    "    'pipe_params' should be a list of tuples consisting of a series of name/transform pairs followed by a name/model, \n",
    "            e.g. [('cvec', CountVectorizer()), ('log', LogisticRegression())]\n",
    "    'grid_params' should be a series of parameters for those transforms and the model in the form of a dictionary,\n",
    "            e.g. {'cvec__ngram_range': [(1,1), (1,2)]}\n",
    "    Be sure the names for the 'pipe_params' and in the 'grid_params match'\n",
    "    '''\n",
    "    \n",
    "    global gs\n",
    "    \n",
    "    pipe = Pipeline(pipe_params)\n",
    "    \n",
    "    gs = GridSearchCV(pipe, grid_params)\n",
    "   \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6074a1dc-b321-4c87-8c83-6419539c16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_grid_njobs(pipe_params, grid_params):\n",
    "    '''\n",
    "    This function is designed to streamline gridsearching.\n",
    "    It returns a gridsearch named 'gs'\n",
    "    'pipe_params' should be a list of tuples consisting of a series of name/transform pairs followed by a name/model, \n",
    "            e.g. [('cvec', CountVectorizer()), ('log', LogisticRegression())]\n",
    "    'grid_params' should be a series of parameters for those transforms and the model in the form of a dictionary,\n",
    "            e.g. {'cvec__ngram_range': [(1,1), (1,2)]}\n",
    "    Be sure the names for the 'pipe_params' and in the 'grid_params match'\n",
    "    '''\n",
    "    \n",
    "    global gs\n",
    "    \n",
    "    pipe = Pipeline(pipe_params)\n",
    "    \n",
    "    gs = GridSearchCV(pipe, grid_params, n_jobs = -1)\n",
    "    \n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe64ddd-8b8e-4fae-9e18-c89170177a71",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "I developed several models in which I didn't eliminate proper names, but as it seemed too easy, per Hank's suggestion, I eliminated the proper names and franchise specific references that appeared in the top 150 words from the overall list and from each show. The models below are tuned with that list. I saved the other models but didn't include them here in order to avoid clutter. Please let me know if you want to see them.\n",
    "\n",
    "Because I did that work, I started my models below from what I found to be the best parameters without the proper name/franchise references removed and tuned from there.\n",
    "\n",
    "I also worked with the proper_names list (found in ['02_EDA_and_Addl_Cleaning.ipynb'](02_EDA_and_Addl_Cleaning.ipynb)), but found that was a little easier than I'd like, too, so I created expanded_stop_words, which I work with here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08f796-73ab-4bf3-bef7-c7d43a0f77fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring Models with Proper Name Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42d5a5b9-eb3f-4c5a-a6e8-ba3f9760e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_names = ['snw', 'discovery', 'force', 'federation', 'darth', 'borg', 'wars', 'anakin',\\\n",
    "                'lightsaber', 'vader', 'picard', 'skywalker', 'sith', 'spock', 'starfleet', 'leia',\\\n",
    "                'star', 'tng', 'ds9', 'reva', 'strange', 'luke', 'series', 'obi', 'clone', 'trek',\\\n",
    "                'enterprise', 'disney', 'wan', 'voyager', 'jedi', 'kenobi', 'tos']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(proper_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7806423-3d2c-4715-b0a8-44acfa6d1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_proper_names = ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia',\\\n",
    "                         'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet',\\\n",
    "                         'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation',\\\n",
    "                         'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf',\\\n",
    "                         'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith',\\\n",
    "                         'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']\n",
    "\n",
    "\n",
    "expanded_stop_words = text.ENGLISH_STOP_WORDS.union(expanded_proper_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454c976-f420-445b-b40c-2aa033dc2ab8",
   "metadata": {},
   "source": [
    "# Summary of Models\n",
    "\n",
    "I explored all four combinations of CountVectorizer, TfidfVectorizer, MultinomialNB, and LogisticRegression\n",
    "\n",
    "As measured by accuracy on test data, the best model is Model 2, which uses CountVectorizer and MultinomialNB. It also had the best fit. The best parameters were:\n",
    "* Cvec max_df:\n",
    "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stop_words, 'nb__alpha': 1.0} Training score: 0.9344524380495604 Test score: 0.8801916932907349\n",
    "\n",
    "As measured by accuracy on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a594c52-412c-4785-9f7c-2301de306222",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 1, TfidfVectorizer, MultinomialNB\n",
    "My best TfidfVectorizer, MultinomialNB model was my best model with proper names, so I started with this model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8309549-fded-4865-9328-229c1be5b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.651350021362305\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'enterprise', 'every', 'toward', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'series', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'themselves', 'should', 'sincere', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'etc', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'besides', 'his', 'ie', 'fifty', 'next', 'against', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'star', 'luke', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'only', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'})}\n",
      "Training score: 0.9483080202504663\n",
      "Test score: 0.8777955271565495\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [stop_words, proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7d8d7212-36a5-4823-94b2-4d4417c7796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "44.001856088638306\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9381827871036504\n",
      "Test score: 0.8586261980830671\n"
     ]
    }
   ],
   "source": [
    "#using expanded_stop_words makes it marginally weaker, but not much, really\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d6c8909a-7db3-421d-b4a6-2de575a6b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "43.8257269859314\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9381827871036504\n",
      "Test score: 0.8586261980830671\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.05, 0.1, 0.2],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8494753f-df41-4876-a858-e91e7e6efa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "13.498803853988647\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.09, 0.1, 0.11],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "162e303c-40d0-4d2c-b38a-ea544bc54cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "6.10121488571167\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.07, 0.08, 0.09],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "047c8595-a91a-4c96-ab56-b8bd4f0dcf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.1546919345855713\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9387156941113776\n",
      "Test score: 0.860223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "82ca2e3e-24cb-444d-abb1-8c24a8d4ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.814697265625e-06\n",
      "2.118048906326294\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "# best without changing alpha\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900, 5_300, 5_700]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79063945-fd16-4562-9d83-2a0432884f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1023550033569336\n",
      "Best parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'})}\n",
      "Training score: 0.9536370903277378\n",
      "Test score: 0.8769968051118211\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL 1\n",
    "\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70e94e6d-1f76-4164-aca3-804f1b43b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.1075940132141113\n",
      "Best parameters: {'nb__alpha': 0.3, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9597655209166001\n",
      "Test score: 0.8761980830670927\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.3, 0.5, 0.7]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657230d4-6897-4d7d-bddd-d628d4b67ade",
   "metadata": {},
   "source": [
    "## Model 1, TfidfVectorizer and MultinomialNB, Best Parameters\n",
    "\n",
    "BBest parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': expanded_stop_words\n",
    "Training score: 0.9536370903277378\n",
    "Test score: 0.8769968051118211"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0657d-bb3c-4833-8489-971285b7bd95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 2, CountVectorizer, MultinomialNB \n",
    "Without proper names removed, the second best model was the optimized CountVectorizer/MultinomialNB model, so I worked through this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9744268b-7353-482f-b072-0a862db87555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.245208740234375e-06\n",
      "43.72111892700195\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.929656274980016\n",
      "Test score: 0.8793929712460063\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "583d64d5-a794-47e6-aa2d-70686852281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-06\n",
      "43.68476104736328\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9357847055688783\n",
      "Test score: 0.8793929712460063\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8bd536-6aa9-4518-886a-1c47dc700e6a",
   "metadata": {},
   "source": [
    "No improvement in test score, less strong fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3bd11487-fb90-42fe-be59-507bef4acd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.9604644775390625e-06\n",
      "43.8722620010376\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "73b9c3e7-5add-498c-9bb5-5856318d6157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "43.96654176712036\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [6_900, 7_000, 7_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6a8e2fa3-3751-4506-85da-fc41a6ccc936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.9141387939453125e-06\n",
      "14.783697843551636\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.24, 0.34, 0.44],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cc9b615e-1edf-4f77-9678-0589776205f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "2.161890983581543\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.2, 0.24, 0.28],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9ad28c60-5bd2-46c2-8cff-fcfb8b3950f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "2.161890983581543\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION WITHOUT CHANGING NB PARAMETERS\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "47049cc8-b068-4acf-a399-77476ca3c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.142659902572632\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'nb__alpha': 1.0}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7aa5e82b-7836-44f5-8d40-37dc44a4ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.0531158447265625e-06\n",
      "2.1283137798309326\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'nb__alpha': 1.0}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION, ALPHA 1.0\n",
    "\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000],\n",
    "    'nb__alpha': [0.9, 1.0, 1.1]    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ada30-acd5-4e62-918d-60072599c62d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Version of Model 2.2, Cvec & NB\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stop_words, 'nb__alpha': 1.0}\n",
    "Training score: 0.9344524380495604\n",
    "Test score: 0.8801916932907349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a325f0-4dee-46e8-9195-2e7399e534c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model 3, CountVectorizer & LogisticRegression\n",
    "Without proper names removed, the optimized CountVectorizer and LogisticRegression combination was the third best, so I worked with that 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "06bbf1a4-07de-43f3-85ad-2e75dc600907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.2159347534179688e-05\n",
      "56.74627709388733\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9669597655209166\n",
      "Test score: 0.8410543130990416\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dd526149-2141-46af-be44-e22ac33a1a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.814697265625e-06\n",
      "59.06657409667969\n",
      "Best parameters: {'cvec__max_df': 0.37, 'cvec__max_features': 4000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.968558486544098\n",
      "Test score: 0.8306709265175719\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [4_000, 4_500, 5_000],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "296c55b7-3f7d-4de1-8695-10222ffa8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-06\n",
      "20.858755111694336\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.84185303514377\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09, 0.19, 0.29],\n",
    "    'cvec__max_features': [4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c616f513-106c-4a14-9b85-987a95327288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "6.810006856918335\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.84185303514377\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.04, 0.09, 0.14],\n",
    "    'cvec__max_features': [4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "88c64436-1265-4517-a345-4aa90b36264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.4066696166992188e-05\n",
      "3.3841030597686768\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.5}\n",
      "Training score: 0.9746869171329603\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f754f696-6d9c-45bf-8770-589823ca2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-07\n",
      "3.591322183609009\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 2.5}\n",
      "Training score: 0.9832134292565947\n",
      "Test score: 0.8434504792332268\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.5, 2.0, 2.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5e19fa96-aa30-4f60-9925-f28c3f596eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.9604644775390625e-06\n",
      "3.9458119869232178\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.7}\n",
      "Training score: 0.9770849986677325\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.3, 1.5, 1.7]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f1e0bc1a-0f9b-42fd-bb9e-5b706dc1c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.76837158203125e-06\n",
      "2.444786787033081\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.5}\n",
      "Training score: 0.9746869171329603\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION OF 3\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.5, 1.6]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a414-f446-460c-b2b2-0af6be9baf80",
   "metadata": {},
   "source": [
    "## Best 1.2 Cvec and LogReg\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stop_words, 'log__C': 1.5}\n",
    "Training score: 0.9746869171329603\n",
    "Test score: 0.8442492012779552"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cf714-9204-46fa-898b-98cee90648ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 4, TfidfVectorizer, LogisticRegression\n",
    "Of the 4 combinations I tried without proper names/show references removed, this was the least effect, so I tried it last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d951a3ee-6341-4d4b-8ec0-a06f487f5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "47.06992697715759\n",
      "Best parameters: {'tvec__max_df': 0.22, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.02, 0.22, 0.42],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dd698831-78d7-4c85-864d-e310b1e99294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.198883056640625e-06\n",
      "47.04106307029724\n",
      "Best parameters: {'tvec__max_df': 0.29, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.9205968558486544\n",
      "Test score: 0.8338658146964856\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.15, 0.22, 0.29],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9e2892d4-96c5-465e-bf26-9a385d327951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "46.90589618682861\n",
      "Best parameters: {'tvec__max_df': 0.2, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9307220889954703\n",
      "Test score: 0.8506389776357828\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.20, 0.22, 0.24],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9dab96f4-df0a-4e0b-b68e-08843993a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "47.65503001213074\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21, 0.22, 0.23],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "469add58-63be-4811-8bd9-5e7a70520300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "16.321935176849365\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [3_400, 4_900, 5_400],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b063b550-d981-4816-96b1-dfe94f228c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "16.49087882041931\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_400, 4_900, 5_300],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "204fd3ed-86bd-4312-8adb-01dea48849bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "36.74275779724121\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_400, 4_900, 5_300],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "887ed111-e97c-496d-b3c9-f53b3c4bd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.0067901611328125e-06\n",
      "5.021714925765991\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0afa978e-d71b-4ed2-9bab-dc9cb21fc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "4.9872589111328125\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9509725552891021\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "578918f3-8024-4e07-b758-b0ee8eb855ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 8.106231689453125e-06\n",
      "5.038173198699951\n",
      "Best parameters: {'log__C': 2.5, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.8698083067092651\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.5, 2.0, 2.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "57de9fdd-3103-4545-9246-264ab68f2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.059906005859375e-06\n",
      "5.052214860916138\n",
      "Best parameters: {'log__C': 1.6, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9547029043431922\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.4, 1.5, 1.6]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81af6687-a446-4ac5-bc2b-8314beec79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.894463300704956\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'})}\n",
      "Training score: 0.9509725552891021\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "#PREFERRED VERSION OF MODEL 4\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e201643-0181-4e64-b53f-d2319efc5dcf",
   "metadata": {},
   "source": [
    "## Best Version of 4.2 Best Log Reg for Inference\n",
    "Best parameters: {'log__C': 1.6, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stop_words\n",
    "Training score: 0.9547029043431922\n",
    "Test score: 0.8618210862619808"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249818d-871b-4b44-bf80-16e2330f1bc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 5, CountVectorizer, RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0347c443-d8b4-4372-8c9a-72ce829afef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "Best parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 1000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos'], 'rf__max_depth': 7, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100}\n",
      "Training score: 0.7708499866773248\n",
      "Test score: 0.713258785942492\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [50, 100, 150],\n",
    "    'rf__max_depth': [3, 5, 7],\n",
    "    'rf__min_samples_split': [3, 5, 7],\n",
    "    'rf__min_samples_leaf': [3, 5, 7]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e520ed6-e831-4563-bb66-56c1f406cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 0.007119894027709961\n",
      "99.50046181678772\n",
      "Best parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 1000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos'], 'rf__max_depth': 9, 'rf__min_samples_leaf': 4, 'rf__min_samples_split': 4, 'rf__n_estimators': 125}\n",
      "Training score: 0.7807087663202771\n",
      "Test score: 0.7228434504792333\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__max_df': [1.0],\n",
    "    'cvec__max_features': [500, 1000],\n",
    "    'rf__n_estimators': [75, 100, 125],\n",
    "    'rf__max_depth': [7, 9],\n",
    "    'rf__min_samples_split': [3, 4],\n",
    "    'rf__min_samples_leaf': [3, 4]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f8bba-e5ba-4304-ad02-c60a353e062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bb84259-c08c-4bd4-a208-af10f32c226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.00438189506531\n",
      "Best parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 750, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos'], 'rf__max_depth': 9, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100}\n",
      "Training score: 0.7881694644284573\n",
      "Test score: 0.731629392971246\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__max_df': [1.0],\n",
    "    'cvec__max_features': [750, 1000],\n",
    "    'rf__n_estimators': [75, 100, 125],\n",
    "    'rf__max_depth': [7, 9],\n",
    "    'rf__min_samples_split': [3, 4],\n",
    "    'rf__min_samples_leaf': [3, 4]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd7050-01af-462c-9c8c-c0538efdb890",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 6, TfidfVecorizer, RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f0c66-2918-4021-88ae-2eebb344c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 0.00506281852722168\n",
      "15.203682661056519\n",
      "Best parameters: {'rf__max_depth': 3, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100, 'tvec__max_df': 0.5, 'tvec__max_features': 1000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.697841726618705\n",
      "Test score: 0.6661341853035144\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'tvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'tvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [100], #50, 150\n",
    "    'rf__max_depth': [3], #5, 7\n",
    "    'rf__min_samples_split': [3], #, 5, 7\n",
    "    'rf__min_samples_leaf': [3] #, 5, 7\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98407f-eb23-4b8a-a8ab-e03723cec443",
   "metadata": {},
   "source": [
    "# Model 7 VotingClassifier with AdaBoosting and GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b7616fc-13b5-4872-b371-50cc38013c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.90030717849731\n",
      "Best Parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 2000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 75, 'vote__gb__n_estimators': 75, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8539834798827605\n",
      "Test Score: 0.7819488817891374\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [50, 75],\n",
    "    'vote__gb__n_estimators': [50, 75]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aea66b7f-c64b-415a-b3d1-07ff9b7070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.36913919448853\n",
      "Best Parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 100, 'vote__gb__n_estimators': 100, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8721023181454837\n",
      "Test Score: 0.799520766773163\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.9, 1.0],\n",
    "    'cvec__max_features': [2_000, 3_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [75, 100],\n",
    "    'vote__gb__n_estimators': [75, 100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26097cf8-88f1-4c92-b687-81384cc3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.05036687850952\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 150, 'vote__gb__n_estimators': 150}\n",
      "Training Score: 0.9046096456168399\n",
      "Test Score: 0.8186900958466453\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, 1.0],\n",
    "    'cvec__max_features': [3_000, 4_000],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [100, 150],\n",
    "    'vote__gb__n_estimators': [100, 150]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f02b914a-1337-4e90-90c7-d3b2ce596380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.90894794464111\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 200, 'vote__gb__n_estimators': 200}\n",
      "Training Score: 0.9184652278177458\n",
      "Test Score: 0.8274760383386581\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, .96],\n",
    "    'cvec__max_features': [3_000, 3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [150, 200],\n",
    "    'vote__gb__n_estimators': [150, 200]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "952465eb-1fff-4249-8d3f-f62726dcb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.20399808883667\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350}\n",
      "Training Score: 0.9581667998934186\n",
      "Test Score: 0.8466453674121406\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [400, 500],\n",
    "    'vote__gb__n_estimators': [300, 350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44d99ca1-8686-4ddc-9df1-d452d7dbbf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.373717069625854\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 500, 'vote__gb__n_estimators': 400}\n",
      "Training Score: 0.9680255795363709\n",
      "Test Score: 0.8354632587859425\n"
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [500, 600],\n",
    "    'vote__gb__n_estimators': [350, 400]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "581334ac-c371-48db-a5ad-ba5a40aa9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.939864158630371\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': 15}\n",
      "Training Score: 0.8862243538502531\n",
      "Test Score: 0.8059105431309904\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    'vote__tree__max_depth': [5, 10, 15],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6529595d-8b24-4cba-951f-30ba38809b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.841071844100952\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.957900346389555\n",
      "Test Score: 0.84185303514377\n"
     ]
    }
   ],
   "source": [
    "#best version of model 7, with cvec__max_features: [3500]\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_000, 3_500, 4_000],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3590afb-0a8c-4531-b7ee-8f735dc599de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.784019947052002\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3400, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'thereby', 'thence', 'mill', 'vader', 'even', 'empty', 'contact', 'enterprise', 'every', 'toward', 'pike', 'among', 'anyone', 'hereupon', 'hereafter', 'front', 'ten', 'few', 'throughout', 'seems', 'other', 'un', 'along', 'skywalker', 'full', 'another', 'seven', 'less', 'last', 'lightsaber', 'after', 'into', 'the', 'bill', 'latterly', 'detail', 'beside', 'interest', 'yet', 'strange', 'more', 'move', 'about', 'towards', 'top', 'may', 'than', 'cry', 'nowhere', 'although', 'formerly', 'most', 'thereupon', 'kenobi', 'within', 'been', 'afterwards', 'as', 'first', 'there', 'own', 'thought', 'done', 'also', 'fill', 'because', 'while', 'still', 'have', 'many', 'will', 'must', 'any', 'becomes', 'others', 'he', 'hers', 'sith', 'fire', 'spock', 'are', 'nobody', 'anyhow', 'twelve', 'hasnt', 'back', 'fifteen', 'someone', 'hereby', 'him', 'by', 'and', 'made', 'but', 'through', 'upon', 'find', 'we', 'trilogy', 'myself', 'in', 'jedi', 'tos', 'never', 'which', 'wherein', 'starfleet', 'over', 'worf', 'themselves', 'should', 'jurati', 'sincere', 'kirk', 'off', 'name', 'therefore', 'cant', 'nor', 'until', 'became', 'something', 'amount', 'himself', 'together', 'seem', 'your', 'too', 'yourself', 'darth', 'with', 'not', 'thin', 'sometimes', 'thick', 'them', 'clone', 'here', 'else', 'an', 'some', 'ever', 'down', 'latter', 'herein', 'out', 'empire', 'palpatine', 'etc', 'republic', 'was', 'whenever', 'being', 'would', 'of', 'behind', 'around', 'co', 'same', 'itself', 'ds9', 'get', 'had', 'several', 'all', 'hundred', 'mine', 'leia', 'whereupon', 'across', 'becoming', 'see', 'mostly', 'onto', 'cannot', 'eg', 'give', 'three', 'beyond', 'st', 'mandalorian', 'one', 'no', 'meanwhile', 'sometime', 'almost', 'already', 'put', 'everyone', 'further', 'moreover', 'again', 'much', 'yourselves', 'forty', 'herself', 'reva', 'thereafter', 'paramount', 'besides', 'his', 'ie', 'fifty', 'boba', 'next', 'inquisitor', 'against', 'maul', 'order', 'snw', 'inc', 'below', 'part', 'however', 'yours', 'whoever', 'me', 'neither', 'wherever', 'luke', 'klingons', 'star', 'beforehand', 'on', 'whence', 'above', 'hence', 'twenty', 'everywhere', 'well', 'under', 'whatever', 'its', 'disney', 'bottom', 'due', 'wan', 'serious', 'rebels', 'only', 'data', 'might', 'amoungst', 'de', 'am', 'could', 'force', 'they', 'four', 'tng', 'whither', 'she', 'voyager', 'during', 'whom', 'now', 'namely', 'borg', 'their', 'were', 'thru', 'therein', 'third', 'fett', 'whose', 'to', 'often', 'between', 'side', 'go', 'except', 'former', 'somehow', 'whereafter', 'two', 'keep', 'discovery', 'least', 'describe', 'klingon', 'both', 'without', 'whereby', 'you', 'indeed', 'why', 'when', 'yoda', 'per', 'wars', 'if', 'alone', 'it', 'elsewhere', 'very', 'from', 'ourselves', 'anything', 'prequels', 'ltd', 'nevertheless', 'us', 'has', 'our', 'or', 'once', 'those', 'whereas', 'eleven', 'so', 'eight', 'captain', 'otherwise', 'seemed', 'be', 'couldnt', 'up', 'picard', 'such', 'always', 'how', 'noone', 'riker', 'is', 'whether', 'for', 'my', 'a', 'please', 'everything', 're', 'these', 'do', 'whole', 'since', 'ours', 'warp', 'anakin', 'system', 'amongst', 'obi', 'trek', 'seeming', 'become', 'perhaps', 'anyway', 'though', 'anywhere', 'before', 'sixty', 'rather', 'then', 'nothing', 'six', 'either', 'where', 'federation', 'her', 'show', 'found', 'somewhere', 'i', 'what', 'at', 'who', 'call', 'thus', 'take', 'nine', 'enough', 'each', 'via', 'can', 'this', 'con', 'none', 'five', 'that'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.9584332533972821\n",
      "Test Score: 0.8394568690095847\n"
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_400, 3_500, 3_600],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf80bf4-b5a9-4fc1-9671-4862139a9c2d",
   "metadata": {},
   "source": [
    "## Model 7, VoteClassifier with AdaBoost, GradientBoost, and DecisionTree\n",
    "\n",
    "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': expanded_stop_words 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
    "Training Score: 0.957900346389555\n",
    "Test Score: 0.84185303514377\n",
    "\n",
    "This is overfit and not as strong as my other two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cecea-6a2c-45bc-a7f5-9b64ae296f45",
   "metadata": {},
   "source": [
    "# Adding in Post Length, Post Word Count, Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e8217-9ad9-4e81-80e9-9e9c6aa7ce73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
