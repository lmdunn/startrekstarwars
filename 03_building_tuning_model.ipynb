{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f913ea-baa9-4b11-81fc-aab2af393652",
   "metadata": {},
   "source": [
    "I based my initial selection for imports off the work we did in the NLP Practice breakfast hour challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "95150329-11b5-4600-b86d-991db9701b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import functions as fun\n",
    "from lists import expanded_stopwords, expanded_proper_names\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0e13433b-bd35-4a58-bac0-3d6b93af2509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>title</th>\n",
       "      <th>all_words</th>\n",
       "      <th>body_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>no_selftext</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1656261965</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>Which version of Klingons will appear in SNW?</td>\n",
       "      <td>Which version of Klingons will appear in SNW?</td>\n",
       "      <td>9.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1656254308</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>On the Gorn and language</td>\n",
       "      <td>On the Gorn and language</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1656248567</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>What are some good things that can be said abo...</td>\n",
       "      <td>What are some good things that can be said abo...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1656238740</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>A Lord of the Rings reference in SNW 1x08</td>\n",
       "      <td>A Lord of the Rings reference in SNW 1x08</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1656238132</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>startrek</td>\n",
       "      <td>The sword props used in SNW 1x08 are replicas ...</td>\n",
       "      <td>The sword props used in SNW 1x08 are replicas ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  created_utc   selftext subreddit_name  \\\n",
       "0           0   1656261965  [removed]       startrek   \n",
       "1           1   1656254308  [removed]       startrek   \n",
       "2           2   1656248567  [removed]       startrek   \n",
       "3           3   1656238740  [removed]       startrek   \n",
       "4           4   1656238132  [removed]       startrek   \n",
       "\n",
       "                                               title  \\\n",
       "0      Which version of Klingons will appear in SNW?   \n",
       "1                           On the Gorn and language   \n",
       "2  What are some good things that can be said abo...   \n",
       "3          A Lord of the Rings reference in SNW 1x08   \n",
       "4  The sword props used in SNW 1x08 are replicas ...   \n",
       "\n",
       "                                           all_words  body_length  \\\n",
       "0      Which version of Klingons will appear in SNW?          9.0   \n",
       "1                           On the Gorn and language          9.0   \n",
       "2  What are some good things that can be said abo...          9.0   \n",
       "3          A Lord of the Rings reference in SNW 1x08          9.0   \n",
       "4  The sword props used in SNW 1x08 are replicas ...          9.0   \n",
       "\n",
       "   title_length  body_word_count  title_word_count  no_selftext  \\\n",
       "0          45.0              1.0               8.0          0.0   \n",
       "1          24.0              1.0               5.0          0.0   \n",
       "2          61.0              1.0              13.0          0.0   \n",
       "3          41.0              1.0               9.0          0.0   \n",
       "4         119.0              1.0              22.0          0.0   \n",
       "\n",
       "   avg_word_length  \n",
       "0         4.625000  \n",
       "1         4.000000  \n",
       "2         3.692308  \n",
       "3         3.333333  \n",
       "4         4.090909  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df = pd.read_csv('data/cleaned_all_text2022-06-29.csv')\n",
    "\n",
    "whole_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f714a9a5-ae0e-41aa-9e65-3e64e41b4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_df[['subreddit_name', 'all_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96423d-bcd5-42bf-b7e1-266ff80f52f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Accuracy\n",
    "\n",
    "Our baseline accuracy is 50.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "55197498-414c-49ce-a663-7450d7628006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startrek    0.509214\n",
       "starwars    0.490786\n",
       "Name: subreddit_name, dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit_name.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3902cbf6-8ad4-4c19-99ec-d544e39ff1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test phrase: my computer computes computationally\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [160]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m test_phrase1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy computer computes computationally\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest phrase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_phrase1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest phrase lemmatized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemmatize_text(test_phrase1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest phrase stemmed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstem_text(test_phrase1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [160]\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_text\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     split_text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m----> 3\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordNetLemmatizer\u001b[49m()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m split_text])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    split_text = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])\n",
    "\n",
    "def stem_text(text):\n",
    "    split_text = text.split()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_text])\n",
    "\n",
    "test_phrase1 = 'my computer computes computationally'\n",
    "print(f'Test phrase: {test_phrase1}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase1)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase1)}')\n",
    "print('')\n",
    "\n",
    "test_phrase2 = 'studies studying cries cry'\n",
    "print(f'Test phrase: {test_phrase2}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase2)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4027de-85f0-40c0-98e7-f4b5642eaaf3",
   "metadata": {},
   "source": [
    "# Prepping Data for Modeling\n",
    "\n",
    "Getting training and test data set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cde5407a-1409-4f29-8a76-abcfb2bb5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209,)\n",
      "y_train shape: (5209,)\n",
      "X_test shape: (1737,)\n",
      "y_test shape: (1737,)\n",
      "\n",
      "Dataframe shape: (6946, 2)\n",
      "\n",
      "y_train value counts: startrek    0.509119\n",
      "starwars    0.490881\n",
      "Name: subreddit_name, dtype: float64\n",
      "y_test value counts: startrek    0.509499\n",
      "starwars    0.490501\n",
      "Name: subreddit_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['all_words']\n",
    "y = df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('')\n",
    "print('y_train value counts:', y_train.value_counts(normalize = True))\n",
    "print('y_test value counts:', y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe64ddd-8b8e-4fae-9e18-c89170177a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General Notes on Modeling\n",
    "## Previous Work\n",
    "I developed several models in which I didn't eliminate proper names, but as it seemed too easy, per Hank's suggestion I eliminated the proper names and franchise specific references that appeared in the top 150 words from the overall list and from each show. Even after using a list of names/terms from the two franchises, I felt it was a little too easy. For that reason, in this notebook, I worked from an expanded list of names/terms from the shows that I eliminated from the models in the form of stopwords.\n",
    "\n",
    "I saved the previous modeling I did without removing show specific names/words and with the shorter list. I didn't include them here in order to avoid clutter. Please let me know if you want to see them.\n",
    "\n",
    "## Notes On These Models\n",
    "As I developed the models in this notebook, I started from the best parameters from that first modeling process as my starting point with these models.\n",
    "\n",
    "In the course of tuning the models in this notebook, I used 'expanded_proper_names' and 'expanded_stopwords'. The first is the longest list of proper names and show specific references I developed. The other is that list combined with the scikit-learn 'english' stopwords. This enabled me to see if the 'english' stopwords helped the models or not.\n",
    "\n",
    "## Observations\n",
    "### On Hyper-Parameters\n",
    "In previous modeling with no or fewer franchise-specific stopwords, I noticed that the best models often had best 'ngram_range' = (1,2) and the 'english' stopwords taken out. Interestingly, and perhaps intuitively, with more of the franchise specific words eliminated, the best 'ngram_range' is (1,1) and the models are tending to perform better with the 'english' stopwords *not* removed from the vectors more frequently than I would have anticipated. Eliminating enough franchise-specific words would logically turn many two word ngrams into gibberish, and it's not illogical that stopwords would become more helpful to modeling in the absence of these other words.\n",
    "\n",
    "### On Tuning Decisions\n",
    "GridSearchCV prefers the best parameters on the training data it's being tuned with. I watched the test score to make parameter decisions, so in some cases I chose parameters that GridSearchCV didn't select as optimal for that reason.\n",
    "\n",
    "#### Accuracy v Star Trek Precision\n",
    "I also did my tuning before I narrowed down the problem statement for the project. Ultimately, especially after having done me full analysis, I want Star Trek precision to be as high as possible. When I did my tuning, I was focused on accuracy. As a final step, I went back and looked at where I started and where I ended with the classification report to see what Star Trek precision was. As it turns out, it improved as accuracy improved, so focusing on Star Trek precision from the start may not actually have mattered. Regardless, I wanted to acknowledge that change in my metric of choice and the reason for it.\n",
    "\n",
    "## Best Models\n",
    "### Model 8, TfidfVectorizer, MultinomialNB, with 'no_selftext'\n",
    "This was the best model as measured by score, with particular focus on test score. It's also the best fit model. I experimented with adding other features, but the 'no_selftext' feature was most predictive (in fact, other features made the model weaker).\n",
    "\n",
    "The parameters:\n",
    "* 'nb__alpha': 0.2 \n",
    "* 'tvec__max_df': 0.7 \n",
    "* 'tvec__max_features': 4800 \n",
    "* 'tvec__ngram_range': (1, 1) \n",
    "* 'tvec__stop_words': expanded_proper_names\n",
    "* 'no_selftext' added to dataframe\n",
    "\n",
    "Training score: 0.9249376079861777\\\n",
    "Test score: 0.8733448474381117\n",
    "\n",
    "### Model 9, TfidfVectorizer, LogisticRegression, with 'no_selftext'\n",
    "This was the best LogisticRegrssion model as measured by score, with particular focus on test score. I'm particularly interested in LogisticRegressions so that I can do some inference and see what I learn. Again, I experimented with adding other features, but the 'no_selftext' feature was most predictive (in fact, other features made the model weaker).\n",
    "\n",
    "The parameters:\n",
    "* 'log__C': 1.4\n",
    "* 'tvec__max_df': 0.12\n",
    "* 'tvec__max_features': 6500\n",
    "* 'tvec__ngram_range': (1, 2)\n",
    "* 'tvec__stop_words': expanded_stopwords\n",
    "* 'no_selftext' added to dataframe\n",
    "\n",
    "Training score: 0.9149548857746208\n",
    "Test score: 0.8480138169257341\n",
    "\n",
    "## Acknowledgements\n",
    "I modeled my work flow initially on the NLP Breakfast Hour, though I think it wound up changing a lot from there.\n",
    "\n",
    "**IMPORTANT: I ran the following models with n_jobs = -1. If you want to run any of them without n_jobs = -1, you can change `'fun.pip_grid_njobs'` to `'fun.pip_grid'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6c823-f4dc-443c-b9fb-822a62d33b9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stemming and Lemmatization\n",
    "In the models without the expanded stopwords, I checked lemmatized and stemmed word lists, but as a rule found that the stop words were more helpful. Between that and the way that I'm removing the stop words, which doesn't play nice with lemmatizing and stemming, I would leave that out, Except that the rubric mentions using them. For that reason, I've included an example, below. It removes none of the stopwords, including the proper name, etc., words, so it outperforms the other models.\n",
    "\n",
    "Because I pulled it from within workflow of my previous work, I was mid-tuning, already having narrowed in on some hyper-parameters that seemed to provide the best results. I did a little tuning, below, just to demonstrate that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e88c11cb-3e78-4307-a198-d01b3ed2a64b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [162]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m gs \u001b[38;5;241m=\u001b[39m fun\u001b[38;5;241m.\u001b[39mpipe_grid_njobs(pipe_params, grid_params)\n\u001b[1;32m     15\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to run\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgs\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 390\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1327\u001b[0m             )\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1203\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1203\u001b[0m         feature_idx \u001b[38;5;241m=\u001b[39m \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[1;32m   1205\u001b[0m             feature_counter[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__preprocessor': [None, fun.lemmatize_text, fun.stem_text],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.18, 0.19, 0.2],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35c02e-3e20-4176-a8ca-5fd246288bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__preprocessor': [None, fun.lemmatize_text, fun.stem_text],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_500, 3_000, 3_500]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb52a92-98ee-4a6d-9a09-51f91a2bbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__preprocessor': [None, fun.lemmatize_text, fun.stem_text],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [3_500, 4_000, 4_500]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a594c52-412c-4785-9f7c-2301de306222",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 1, TfidfVectorizer, MultinomialNB\n",
    "My best TfidfVectorizer, MultinomialNB model was my best model with proper names, so I started with this model here.\n",
    "\n",
    "Best parameters: {'nb__alpha': 0.25, 'tvec__max_df': 0.7, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': expanded_proper_names}\n",
    "Training score: 0.9199462468803993\n",
    "Test score: 0.8514680483592401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d7212-36a5-4823-94b2-4d4417c7796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550546c-f0f0-4257-90bd-c3d656da2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b027a-415f-4d95-b7fa-fe434fab7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.3, 0.5, 0.7]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f6fa3-44d5-4c7e-890a-ef0170710c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.2, 0.3, 0.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99db18-3f2a-470b-8038-089c7e7a5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900],\n",
    "    'nb__alpha': [0.25, 0.3, 0.35]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfe131-f91f-4386-bbf8-4fc591d8a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_700, 4_900, 5_100],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2964d6-0f5c-4f89-bd36-b4573dc56f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a705d43-c314-4603-8303-114086cb6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [4_700, 4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0c565-32aa-4874-858b-8f19aea14ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.8, 0.9, 1],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910b034-63d1-48d7-857f-6efc5d3d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.7, 0.8],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ba9f5-4f17-4a7e-a6a6-7c67ac513e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2),(1,3)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.7],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c003077-ca18-4839-ad27-b46e2b89debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST VERSION MEASURED BY ACCURACY SCORE\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_proper_names],\n",
    "    'tvec__max_df': [0.7],\n",
    "    'tvec__max_features': [4_800],\n",
    "    'nb__alpha': [0.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4a58f-5b11-45c6-9f0c-471a49630305",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0657d-bb3c-4833-8489-971285b7bd95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 2, CountVectorizer, MultinomialNB \n",
    "Without proper names removed, the second best model was the optimized CountVectorizer/MultinomialNB model, so I worked through this next.\n",
    "\n",
    "Time to run 0.4991631507873535\n",
    "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_proper_names, 'nb__alpha': 0.5}\n",
    "Training score: 0.8871184488385486\n",
    "Test score: 0.8457109959700633\n",
    "\n",
    "This is the better fit of the two top performing models (performance being measured by Test score (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744268b-7353-482f-b072-0a862db87555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22269d76-fa7c-4cc5-adf2-3c63826cd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1bcc0-9242-44eb-b6ec-451eae6ebed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.2, 0.3, 0.4],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd85b2b-4439-4fbe-a495-6273fbb17344",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.4],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9904a0-ba71-47d3-be7d-835fc7be75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.35],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd3422-d34b-4b9c-9157-c867bc4b4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.33, 0.34],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4306e219-814c-46db-b984-1d26919888cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 35.80676507949829\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68146a48-8936-4fbd-85ca-f779787c2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 33.66346979141235\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8869264734114033\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23029b30-879a-4fc5-9243-5ce265281e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 35.17246603965759\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_400, 6_500, 6_600],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e823130a-8d10-47ee-bf5c-00f06faa2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 35.18378305435181\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6800, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8880783259742753\n",
      "Test score: 0.844559585492228\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600, 6_700, 6_800],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3229bf3-945d-4e07-93b0-8b6456ec9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 25.63590669631958\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600, 6_700],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f0b711d-3f33-48d1-b121-23a7344db0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 13.629853963851929\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.25}\n",
      "Training score: 0.8905740065271646\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.25, 0.5, 0.75]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc86ccf6-e19e-45c7-95a0-ad362f84f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 12.209612846374512\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.6}\n",
      "Training score: 0.8861585717028221\n",
      "Test score: 0.8451352907311457\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.4, 0.5, 0.6]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57091b9a-a795-45b7-bf46-da348e21f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run 0.4997067451477051\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6600, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'nb__alpha': 0.5}\n",
      "Training score: 0.8871184488385486\n",
      "Test score: 0.8457109959700633\n"
     ]
    }
   ],
   "source": [
    "#BEST PERFORMANCE, MODEL 2\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_proper_names],\n",
    "    'cvec__max_df': [0.34],\n",
    "    'cvec__max_features': [6_600],\n",
    "    'nb__alpha': [0.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print('Time to run', time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0b0dacd-04dc-4a72-98af-8c04e2956574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.85      0.85      0.85       885\n",
      "    starwars       0.85      0.84      0.84       852\n",
      "\n",
      "    accuracy                           0.85      1737\n",
      "   macro avg       0.85      0.85      0.85      1737\n",
      "weighted avg       0.85      0.85      0.85      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a325f0-4dee-46e8-9195-2e7399e534c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 3, CountVectorizer & LogisticRegression\n",
    "Without proper names removed, the optimized CountVectorizer and LogisticRegression combination was the third best, so I worked with that 3rd.\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stopwords, 'log__C': 1.1}\n",
    "Training score: 0.9537339220579766\n",
    "Test score: 0.8215313759355211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06bbf1a4-07de-43f3-85ad-2e75dc600907",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [118]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m gs \u001b[38;5;241m=\u001b[39m fun\u001b[38;5;241m.\u001b[39mpipe_grid_njobs(pipe_params, grid_params)\n\u001b[1;32m     15\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgs\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d8347887-c9fe-4f33-b6c0-7db621ca7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.84      0.79      0.81       885\n",
      "    starwars       0.79      0.84      0.82       852\n",
      "\n",
      "    accuracy                           0.82      1737\n",
      "   macro avg       0.82      0.82      0.82      1737\n",
      "weighted avg       0.82      0.82      0.82      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a72a90d4-c6fe-47b3-82be-a758f681f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.90738201141357\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9424073718564023\n",
      "Test score: 0.8105929763960852\n"
     ]
    }
   ],
   "source": [
    "# since the model was overfit, I'm going to try making C smaller\n",
    "\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7dbaa34-9d05-4c0a-99fd-9bef369f0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.12069201469421\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 5500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9479746592436168\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [4_500, 5_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa8d5c13-2bff-4041-98dd-b80c8ddea594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.45565509796143\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92739488-6418-484a-8246-a218a402bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.09251093864441\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b04ddb0-5135-42ef-8709-0aa53156b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.67308592796326\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_250, 6_500, 6_750],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0db07669-e7c5-4652-9438-205cd8c7131f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.69028520584106\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9508542906507967\n",
      "Test score: 0.8140472078295913\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [6_400, 6_500, 6_600],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7ba5baa-db3b-4e38-9c43-9c55bea77cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.734118938446045\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 0.5}\n",
      "Training score: 0.93511230562488\n",
      "Test score: 0.8146229130685089\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09, 0.19, 0.29],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b39a1daf-992a-4c24-bd23-344c71d08825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.4624183177948\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 0.5}\n",
      "Training score: 0.93511230562488\n",
      "Test score: 0.8146229130685089\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.04, 0.09, 0.14],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b84bf5a5-560e-479e-9bb1-a9067bd91365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.122130155563354\n",
      "Best parameters: {'cvec__max_df': 0.08, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.0}\n",
      "Training score: 0.9564215780380111\n",
      "Test score: 0.8134715025906736\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.08, 0.09, 0.10],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6e7a237-350f-466b-9c0d-54000a5ccb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.855623006820679\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [0.9, 1.0, 1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5812d6b6-6d05-40d3-bff8-274d76a962f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.006916284561157\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [1.1, 1.2]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f4cd2a7-7292-49e1-8990-9c224c97b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.244507074356079\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'log__C': 1.1}\n",
      "Training score: 0.9537339220579766\n",
      "Test score: 0.8215313759355211\n"
     ]
    }
   ],
   "source": [
    "#BEST MODEL 3\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [6_500],\n",
    "    'log__C': [1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "57db6ccc-f328-455c-b756-ac7f694b2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.85      0.80      0.82       885\n",
      "    starwars       0.80      0.85      0.82       852\n",
      "\n",
      "    accuracy                           0.82      1737\n",
      "   macro avg       0.82      0.82      0.82      1737\n",
      "weighted avg       0.82      0.82      0.82      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cf714-9204-46fa-898b-98cee90648ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 4, TfidfVectorizer, LogisticRegression\n",
    "Of the 4 combinations I tried without proper names/show references removed, this was the least effect, so I tried it last.\n",
    "\n",
    "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stopwords}\n",
    "Training score: 0.9383758878863505\n",
    "Test score: 0.8307426597582038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d951a3ee-6341-4d4b-8ec0-a06f487f5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 40.70953893661499\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.22, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9306968708005375\n",
      "Test score: 0.8290155440414507\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.02, 0.22, 0.42],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "439f7e1b-3c3e-4f74-862d-85177e258cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.84      0.82      0.83       885\n",
      "    starwars       0.82      0.84      0.83       852\n",
      "\n",
      "    accuracy                           0.83      1737\n",
      "   macro avg       0.83      0.83      0.83      1737\n",
      "weighted avg       0.83      0.83      0.83      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfaa34ff-d046-4b34-ba26-fc8bc2931790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 125.62058281898499\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12, 0.22, 0.32],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9805426c-c43c-40bc-8857-67f657340c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 116.72513484954834\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.07, 0.12, 0.17],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "986395d0-b460-451a-90ac-c84f4bbc9361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 112.65803599357605\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.10, 0.12, 0.14],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ded6db72-423b-4ee3-9dd5-3c841cd7cd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 123.3748459815979\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9324246496448455\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.11, 0.12, 0.13],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5e5311c-0020-4eca-85ca-e7b173bdddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 43.78461194038391\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [3_400, 4_900, 6_400],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e9e76603-3ccc-4894-bff2-e82c92ea8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 44.64078903198242\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [5_400, 6_400, 7_400],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39856741-1d4e-4fb5-8e3f-07dc92e85272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 40.244284868240356\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6400, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9387598387406412\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [5_900, 6_400, 6_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f21e02a-ad5d-4dae-96bc-e5af1bc4c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 39.50519895553589\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9391437895949318\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_300, 6_400, 6_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c416fc73-d606-40e4-975c-a921450ed5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 25.87854313850403\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9391437895949318\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500, 6_600],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5483b8a6-bba3-4ffa-be3a-c4580f282929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 16.178356885910034\n",
      "Best parameters: {'log__C': 1.75, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.943367248992129\n",
      "Test score: 0.8295912492803684\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.25, 1.5, 1.75]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db1d5e4b-18f0-4da2-8e2a-84b18ec5437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 5.963215112686157\n",
      "Best parameters: {'log__C': 1.0, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9293530428105202\n",
      "Test score: 0.8290155440414507\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78e7a670-d549-4396-bad6-86e86f1d0da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 5.358742952346802\n",
      "Best parameters: {'log__C': 1.25, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9360721827606067\n",
      "Test score: 0.8301669545192861\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.25]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d147c19-5508-4783-9896-2edb85c82408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 5.662249803543091\n",
      "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9383758878863505\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords, expanded_proper_names],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2eadd2c-7783-41ed-8b65-df0b903c950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 1.0923049449920654\n",
      "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.9383758878863505\n",
      "Test score: 0.8307426597582038\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION OF MODEL 4\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stopwords],\n",
    "    'tvec__max_df': [0.12],\n",
    "    'tvec__max_features': [6_500],\n",
    "    'log__C': [1.4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0999969c-d565-485f-93db-fb365a0fb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.84      0.82      0.83       885\n",
      "    starwars       0.82      0.84      0.83       852\n",
      "\n",
      "    accuracy                           0.83      1737\n",
      "   macro avg       0.83      0.83      0.83      1737\n",
      "weighted avg       0.83      0.83      0.83      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249818d-871b-4b44-bf80-16e2330f1bc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 5, CountVectorizer, RandomForest\n",
    "\n",
    "Because the performance of this model is so much weaker than above, I didn't spend much time trying to tune it.\n",
    "\n",
    "It also takes a long time to run, so when I decided to come back and measure for Star Trek precision, I opted not to run it/the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0347c443-d8b4-4372-8c9a-72ce829afef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "4320 fits failed out of a total of 4320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3456 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 981, in check_X_y\n",
      "    check_consistent_length(X, y)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [12, 4167]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "864 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 327, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 981, in check_X_y\n",
      "    check_consistent_length(X, y)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [12, 4168]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12, 5209]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [173]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m gs \u001b[38;5;241m=\u001b[39m fun\u001b[38;5;241m.\u001b[39mpipe_grid_njobs(pipe_params, grid_params)\n\u001b[1;32m     20\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, gs\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 327\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m--> 981\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12, 5209]"
     ]
    }
   ],
   "source": [
    "#THIS MODEL IS EXTREMELY WELL FIT BUT IT'S NOT NEARLY AS ACCURATE AS THE ONES ABOVE.\n",
    "\n",
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [50, 100, 150],\n",
    "    'rf__max_depth': [3, 5],\n",
    "    'rf__min_samples_split': [3, 5],\n",
    "    'rf__min_samples_leaf': [3, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31fb89d4-d5e3-4a28-abc6-888a39cf9bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [126]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m gs \u001b[38;5;241m=\u001b[39m fun\u001b[38;5;241m.\u001b[39mpipe_grid_njobs(pipe_params, grid_params)\n\u001b[1;32m     18\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to run:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, gs\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'cvec__max_features': [2_250, 2_500],\n",
    "    'rf__n_estimators': [200],\n",
    "    'rf__max_depth': [7],\n",
    "    'rf__min_samples_split': [3, 5],\n",
    "    'rf__min_samples_leaf': [3, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd7050-01af-462c-9c8c-c0538efdb890",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 6, TfidfVecorizer, RandomForest\n",
    "\n",
    "Again, this model is so much weaker than above, with no obvious factors to adjust to dramatically improve it, so I stopped tuning it pretty quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "344f0c66-2918-4021-88ae-2eebb344c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 19.71072292327881\n",
      "Best parameters: {'rf__max_depth': 3, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100, 'tvec__max_df': 0.5, 'tvec__max_features': 2500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme']}\n",
      "Training score: 0.6997504319447111\n",
      "Test score: 0.6799078871617732\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'tvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'tvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [100], #50, 150\n",
    "    'rf__max_depth': [3], #5, 7\n",
    "    'rf__min_samples_split': [3], #, 5, 7\n",
    "    'rf__min_samples_leaf': [3] #, 5, 7\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96832b02-3904-46d4-a768-83fb5cb07805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.63      0.92      0.75       885\n",
      "    starwars       0.84      0.43      0.57       852\n",
      "\n",
      "    accuracy                           0.68      1737\n",
      "   macro avg       0.73      0.68      0.66      1737\n",
      "weighted avg       0.73      0.68      0.66      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dbb1c863-5c99-47a7-84be-bb12be9ddd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 84.5688738822937\n",
      "Best parameters: {'rf__max_depth': 7, 'rf__min_samples_leaf': 3, 'rf__min_samples_split': 3, 'rf__n_estimators': 100, 'tvec__max_df': 1.0, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'})}\n",
      "Training score: 0.741601075062392\n",
      "Test score: 0.7184801381692574\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'tvec__max_df': [0.1, 0.5, 1.0],\n",
    "    'tvec__max_features': [1_000, 2_500, 5_000],\n",
    "    'rf__n_estimators': [100, 150], #50, 150\n",
    "    'rf__max_depth': [5, 7],\n",
    "    'rf__min_samples_split': [3], #, 5, 7\n",
    "    'rf__min_samples_leaf': [3] #, 5, 7\n",
    "    \n",
    "}\n",
    "\n",
    "gs = fun.pipe_grid_njobs(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Time to run:\", time.time()-t0)\n",
    "\n",
    "print('Best parameters:', gs.best_params_)\n",
    "print('Training score:', gs.score(X_train, y_train))\n",
    "print('Test score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "38c5b7d1-bfc0-4b1f-b1ae-be21b3d004cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.67      0.90      0.77       885\n",
      "    starwars       0.84      0.53      0.65       852\n",
      "\n",
      "    accuracy                           0.72      1737\n",
      "   macro avg       0.75      0.71      0.71      1737\n",
      "weighted avg       0.75      0.72      0.71      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98407f-eb23-4b8a-a8ab-e03723cec443",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 7 VotingClassifier with AdaBoosting and GradientBoosting\n",
    "\n",
    "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': expanded_stopwords, 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
    "Training Score: 0.8957573430600884\n",
    "Test Score: 0.7910189982728842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3b7616fc-13b5-4872-b371-50cc38013c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.44944381713867\n",
      "Best Parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 2000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'vote__ada__n_estimators': 75, 'vote__gb__n_estimators': 75, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8107122288347092\n",
      "Test Score: 0.7328727691421992\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.5, 1.0],\n",
    "    'cvec__max_features': [1_000, 2_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [50, 75],\n",
    "    'vote__gb__n_estimators': [50, 75]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2da2e186-e394-48e0-af2a-cd8f96035b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.80      0.64      0.71       885\n",
      "    starwars       0.69      0.83      0.75       852\n",
      "\n",
      "    accuracy                           0.73      1737\n",
      "   macro avg       0.74      0.73      0.73      1737\n",
      "weighted avg       0.74      0.73      0.73      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aea66b7f-c64b-415a-b3d1-07ff9b7070ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219.94416499137878\n",
      "Best Parameters: {'cvec__max_df': 1.0, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'vote__ada__n_estimators': 100, 'vote__gb__n_estimators': 100, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.8174313687847955\n",
      "Test Score: 0.7484168105929764\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.9, 1.0],\n",
    "    'cvec__max_features': [2_000, 3_000],\n",
    "    'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [75, 100],\n",
    "    'vote__gb__n_estimators': [75, 100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26097cf8-88f1-4c92-b687-81384cc3bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161.5521092414856\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'inquisitor', 'trek', 'enterprise', 'tos', 'padme'], 'vote__ada__n_estimators': 150, 'vote__gb__n_estimators': 150}\n",
      "Training Score: 0.8343252063735842\n",
      "Test Score: 0.7547495682210709\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, 1.0],\n",
    "    'cvec__max_features': [3_000, 4_000],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [100, 150],\n",
    "    'vote__gb__n_estimators': [100, 150]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b914a-1337-4e90-90c7-d3b2ce596380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.3765046596527\n",
      "Best Parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'vote__ada__n_estimators': 200, 'vote__gb__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_proper_names, expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.95, .96],\n",
    "    'cvec__max_features': [3_000, 3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [150, 200],\n",
    "    'vote__gb__n_estimators': [150, 200]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "952465eb-1fff-4249-8d3f-f62726dcb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.944379091262817\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350}\n",
      "Training Score: 0.8946054904972164\n",
      "Test Score: 0.7938975244674726\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [400, 500],\n",
    "    'vote__gb__n_estimators': [300, 350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44d99ca1-8686-4ddc-9df1-d452d7dbbf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.19376277923584\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'vote__ada__n_estimators': 600, 'vote__gb__n_estimators': 400}\n",
      "Training Score: 0.9226339028604339\n",
      "Test Score: 0.8019573978123201\n"
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    #'vote__tree__max_depth': [None, 5],\n",
    "    'vote__ada__n_estimators': [500, 600],\n",
    "    'vote__gb__n_estimators': [350, 400]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "581334ac-c371-48db-a5ad-ba5a40aa9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.08287000656128\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3500, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': 15}\n",
      "Training Score: 0.8352850835093109\n",
      "Test Score: 0.7789291882556131\n"
     ]
    }
   ],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_500],\n",
    "    'vote__tree__max_depth': [5, 10, 15],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6529595d-8b24-4cba-951f-30ba38809b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.01622486114502\n",
      "Best Parameters: {'cvec__max_df': 0.96, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': frozenset({'along', 'indeed', 'a', 'because', 'would', 'everything', 'this', 'could', 'eight', 'republic', 'found', 'show', 'other', 'too', 'as', 'when', 'us', 'nevertheless', 'one', 'tos', 'whereas', 'are', 'whither', 'next', 'almost', 'together', 'twelve', 'top', 'order', 'own', 'nor', 'much', 'myself', 'tng', 'paramount', 'obi', 'amongst', 'such', 'fill', 'might', 'same', 'thereupon', 'couldnt', 'noone', 'they', 'whence', 'whether', 'hundred', 'sixty', 'am', 'either', 'here', 'federation', 'force', 'them', 'above', 'eg', 'below', 'still', 'anyhow', 'trek', 'otherwise', 'twenty', 'keep', 'even', 'full', 'nothing', 'fifteen', 'you', 'enough', 'himself', 'cry', 'starfleet', 'whose', 'ours', 'else', 'describe', 'bottom', 'further', 'amount', 'every', 'how', 'therein', 'becoming', 'perhaps', 'can', 'yourself', 'another', 'front', 'seeming', 'but', 'my', 'were', 'onto', 'warp', 'serious', 'not', 'something', 'jurati', 'and', 'five', 'around', 'into', 'part', 'skywalker', 'of', 'then', 'me', 'cant', 'clone', 'why', 'under', 'side', 'un', 'see', 'forty', 'first', 'again', 'behind', 'thereby', 'due', 'seven', 'others', 'empty', 'rather', 'snw', 'must', 'reva', 'boba', 'jedi', 'seem', 'its', 'has', 'so', 'whom', 'ltd', 'often', 'somehow', 'alone', 'kenobi', 'across', 'by', 'her', 'least', 'we', 'thick', 'riker', 'ourselves', 'out', 'thus', 'to', 'moreover', 'thence', 'whereafter', 'wars', 'palpatine', 'everywhere', 'herself', 'on', 'once', 'captain', 'down', 'go', 'will', 'he', 'she', 'wherein', 'latter', 'neither', 'mill', 'now', 'besides', 'from', 'spock', 'being', 'less', 'sith', 'until', 'toward', 'discovery', 'bill', 'lightsaber', 'whoever', 'rebels', 'up', 'latterly', 'beforehand', 'star', 'get', 'beyond', 'themselves', 'all', 'his', 'con', 'may', 'made', 'name', 'inquisitor', 'last', 'both', 'move', 'detail', 'put', 'than', 'whatever', 'if', 'nobody', 'however', 'very', 'became', 'hers', 'borg', 'which', 'darth', 'vader', 'mine', 'wherever', 'those', 'please', 'their', 'already', 'over', 'trilogy', 'among', 'without', 're', 'against', 'be', 'enterprise', 'been', 'nine', 'no', 'seemed', 'the', 'yet', 'most', 'therefore', 'give', 'anakin', 'beside', 'do', 'some', 'ds9', 'upon', 'i', 'before', 'anything', 'was', 'whole', 'these', 'find', 'hereby', 'more', 'only', 'whereupon', 'thru', 'sincere', 'call', 'former', 'off', 'though', 'anyway', 'since', 'while', 'anywhere', 'pike', 'take', 'sometimes', 'ever', 'someone', 'ten', 'itself', 'luke', 'with', 'two', 'six', 'mandalorian', 'amoungst', 'where', 'third', 'becomes', 'picard', 'few', 'each', 'back', 'have', 'that', 'several', 'fett', 'seems', 'cannot', 'done', 'thin', 'inc', 'always', 'empire', 'throughout', 'contact', 'disney', 'never', 'or', 'data', 'is', 'your', 'become', 'per', 'system', 'had', 'any', 'ie', 'etc', 'who', 'what', 'an', 'at', 'within', 'strange', 'eleven', 'klingons', 'hasnt', 'hereupon', 'klingon', 'none', 'everyone', 'nowhere', 'there', 'three', 'between', 'interest', 'sometime', 'about', 'except', 'namely', 'after', 'whereby', 'leia', 'hereafter', 'should', 'maul', 'formerly', 'four', 'padme', 'elsewhere', 'him', 'kirk', 'towards', 'somewhere', 'thereafter', 'co', 'whenever', 'many', 'our', 'fifty', 'for', 'it', 'voyager', 'hence', 'well', 'although', 'fire', 'herein', 'wan', 'yourselves', 'afterwards', 'yoda', 'via', 'st', 'de', 'meanwhile', 'in', 'through', 'worf', 'mostly', 'during', 'yours', 'anyone', 'also'}), 'vote__ada__n_estimators': 400, 'vote__gb__n_estimators': 350, 'vote__tree__max_depth': None}\n",
      "Training Score: 0.894413515070071\n",
      "Test Score: 0.7875647668393783\n"
     ]
    }
   ],
   "source": [
    "#best version of model 7, with cvec__max_features: [3500]. \n",
    "# Also, better precision scores than accuracy scores, so overperforming in that regard\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_000, 3_500, 4_000],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eec8519f-a3cc-4736-9aea-d40d1fc65b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.85      0.71      0.77       885\n",
      "    starwars       0.74      0.87      0.80       852\n",
      "\n",
      "    accuracy                           0.79      1737\n",
      "   macro avg       0.80      0.79      0.79      1737\n",
      "weighted avg       0.80      0.79      0.79      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c3590afb-0a8c-4531-b7ee-8f735dc599de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [133]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe, param_grid \u001b[38;5;241m=\u001b[39m vote_params, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m,gs\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#above is better\n",
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "vote_params = {\n",
    "    'cvec__stop_words': [expanded_stopwords],\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__max_df': [0.96],\n",
    "    'cvec__max_features': [3_400, 3_500, 3_600],\n",
    "    'vote__tree__max_depth': [None],\n",
    "    'vote__ada__n_estimators': [400],\n",
    "    'vote__gb__n_estimators': [350]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = Pipeline([('cvec', CountVectorizer()),\n",
    "                 ('vote', vote)])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = vote_params, cv = 3, n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print('Best Parameters:',gs.best_params_)\n",
    "print('Training Score:', gs.score(X_train, y_train))\n",
    "print(('Test Score:'), gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cecea-6a2c-45bc-a7f5-9b64ae296f45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 8, TfidfVectorizer, MultinomialNB, Title and Body Lengths and Wordcounts\n",
    "Using best TfidfVectorizer parameters from above\\:\n",
    "'nb__alpha': 0.25\\ \n",
    "'tvec__max_df': 0.7\\ \n",
    "'tvec__max_features': 4800\\ \n",
    "'tvec__ngram_range': (1, 1)\\ \n",
    "'tvec__stop_words': expanded_proper_names\\\n",
    "Training score: 0.9199462468803993\\\n",
    "Test score: 0.8514680483592401\n",
    "\n",
    "Best Result with just 'no_selftext' added, alpha = .2:\\\n",
    "Training score: 0.9249376079861777\\\n",
    "Test score: 0.8733448474381117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16aa3696-1c7d-472a-a05e-1ccb8fd03609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 11)\n",
      "X_test shape: (1737, 11)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 12)\n",
      "X_test reset index shape: (1737, 12)\n",
      "vectorized_train concatenated shape: (5209, 4805)\n",
      "vectorized_test concatenated shape: (1737, 4805)\n",
      "vectorized_train corrected index shape: (5209, 4804)\n",
      "vectorized_test corrected index shape: (1737, 4804)\n",
      "Training score: 0.5371472451526205\n",
      "Test score: 0.5371329879101899\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'body_length', 'title_length', 'body_word_count', 'title_word_count']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'body_length', 'title_length', 'body_word_count', 'title_word_count']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.25)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ddffb44c-04b2-4fc8-8441-9c778a9a95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.58      0.33      0.42       885\n",
      "    starwars       0.52      0.75      0.61       852\n",
      "\n",
      "    accuracy                           0.54      1737\n",
      "   macro avg       0.55      0.54      0.52      1737\n",
      "weighted avg       0.55      0.54      0.52      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = nb.predict(vectorized_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff47f1a1-6701-4b28-a6c6-6b3eddaf6887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9235937799961605\n",
      "Test score: 0.8716177317213587\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.25)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "87b3df1d-6a10-49af-8352-c053e610ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.88      0.86      0.87       885\n",
      "    starwars       0.86      0.88      0.87       852\n",
      "\n",
      "    accuracy                           0.87      1737\n",
      "   macro avg       0.87      0.87      0.87      1737\n",
      "weighted avg       0.87      0.87      0.87      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = nb.predict(vectorized_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a7501ac-1519-4c96-995b-8e9bc80a9cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9170666154732194\n",
      "Test score: 0.8658606793321819\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.5)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b627101a-0d13-4871-b6e5-3197b2399791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9220579765789979\n",
      "Test score: 0.869314910765688\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.3)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a570724c-c538-4db0-b348-5001999b5092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9249376079861777\n",
      "Test score: 0.8733448474381117\n"
     ]
    }
   ],
   "source": [
    "# BEST VERSION\n",
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.2)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb04c846-0dca-48af-9f85-480493bd2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.88      0.87      0.87       885\n",
      "    starwars       0.86      0.88      0.87       852\n",
      "\n",
      "    accuracy                           0.87      1737\n",
      "   macro avg       0.87      0.87      0.87      1737\n",
      "weighted avg       0.87      0.87      0.87      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = nb.predict(vectorized_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e193965-555c-4cd7-ad50-9e0f9a46ca0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9272413131119217\n",
      "Test score: 0.8733448474381117\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.15)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b3e185e-9d0f-486a-8e7b-14c0dc164402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 4800)\n",
      "vectorized_test shape: (1737, 4800)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 4802)\n",
      "vectorized_test concatenated shape: (1737, 4802)\n",
      "vectorized_train corrected index shape: (5209, 4801)\n",
      "vectorized_test corrected index shape: (1737, 4801)\n",
      "Training score: 0.9301209445191015\n",
      "Test score: 0.872769142199194\n"
     ]
    }
   ],
   "source": [
    "# WITH JUST 'no_selftext'\n",
    "# ADJUSTING ALPHA\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.7, max_features=4_800, stop_words = expanded_proper_names)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "\n",
    "nb.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', nb.score(vectorized_train, y_train))\n",
    "print('Test score:', nb.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b64769-a426-4d00-8504-acb3788d0cb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model 9, TfidfVectorizer, LogisticRegression, Adding Features\n",
    "Using best TfidfVectorizer parameters from above\n",
    "Best parameters: {'log__C': 1.4, 'tvec__max_df': 0.12, 'tvec__max_features': 6500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stopwords} Training score: 0.9383758878863505 Test score: 0.8307426597582038\n",
    "\n",
    "Best result occurs with just adding 'no_selftext':\n",
    "Training score: 0.9149548857746208\n",
    "Test score: 0.8480138169257341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aea82568-b476-477f-a723-0cfaefc64145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 11)\n",
      "X_test shape: (1737, 11)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 12)\n",
      "X_test reset index shape: (1737, 12)\n",
      "vectorized_train concatenated shape: (5209, 6505)\n",
      "vectorized_test concatenated shape: (1737, 6505)\n",
      "vectorized_train corrected index shape: (5209, 6504)\n",
      "vectorized_test corrected index shape: (1737, 6504)\n",
      "Training score: 0.9251295834133231\n",
      "Test score: 0.8244099021301093\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'body_length', 'title_length', 'body_word_count', 'title_word_count']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'body_length', 'title_length', 'body_word_count', 'title_word_count']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3a7a992c-325f-4e28-923f-dcc81327d6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.83      0.82      0.83       885\n",
      "    starwars       0.82      0.83      0.82       852\n",
      "\n",
      "    accuracy                           0.82      1737\n",
      "   macro avg       0.82      0.82      0.82      1737\n",
      "weighted avg       0.82      0.82      0.82      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = log.predict(vectorized_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "559944d4-ad7a-4870-b79b-d6b49a661823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 204.2449049949646\n",
      "Best paramaters: {'log__C': 1.5}\n",
      "Training score: 0.9251295834133231\n",
      "Test score: 0.82671272308578\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('log', LogisticRegression(max_iter = 10_000))])\n",
    "\n",
    "pipe_params = {\n",
    "    'log__C': [0.5, 1, 1.5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid = pipe_params,\n",
    "                 n_jobs=-1)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(vectorized_train, y_train)\n",
    "print('Time to run:', time.time()-t0)\n",
    "\n",
    "print('Best paramaters:', gs.best_params_)\n",
    "print('Training score:', gs.score(vectorized_train, y_train))\n",
    "print('Test score:', gs.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1d488ab4-dbe2-4de7-a91a-68cd6213f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6502)\n",
      "vectorized_test concatenated shape: (1737, 6502)\n",
      "vectorized_train corrected index shape: (5209, 6501)\n",
      "vectorized_test corrected index shape: (1737, 6501)\n",
      "Training score: 0.9149548857746208\n",
      "Test score: 0.8480138169257341\n"
     ]
    }
   ],
   "source": [
    "# BEST RESULT\n",
    "\n",
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9afed4d3-cae1-4629-bb1d-72f2ac977faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    startrek       0.83      0.88      0.85       885\n",
      "    starwars       0.87      0.82      0.84       852\n",
      "\n",
      "    accuracy                           0.85      1737\n",
      "   macro avg       0.85      0.85      0.85      1737\n",
      "weighted avg       0.85      0.85      0.85      1737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = log.predict(vectorized_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f69dc48f-3cd4-450a-9d67-b3a43a74a3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 10)\n",
      "X_test shape: (1737, 10)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 11)\n",
      "X_test reset index shape: (1737, 11)\n",
      "vectorized_train concatenated shape: (5209, 6503)\n",
      "vectorized_test concatenated shape: (1737, 6503)\n",
      "vectorized_train corrected index shape: (5209, 6502)\n",
      "vectorized_test corrected index shape: (1737, 6502)\n",
      "Training score: 0.9130351315031676\n",
      "Test score: 0.8474381116868164\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext', 'avg_word_length']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext', 'avg_word_length']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "97c612a7-0e5d-440f-af69-fe692f441805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5209, 11)\n",
      "X_test shape: (1737, 11)\n",
      "y_train shape: (5209,)\n",
      "y_test shape: (1737,)\n",
      "vectorized_train shape: (5209, 6500)\n",
      "vectorized_test shape: (1737, 6500)\n",
      "X_train reset index shape: (5209, 12)\n",
      "X_test reset index shape: (1737, 12)\n",
      "vectorized_train concatenated shape: (5209, 6503)\n",
      "vectorized_test concatenated shape: (1737, 6503)\n",
      "vectorized_train corrected index shape: (5209, 6502)\n",
      "vectorized_test corrected index shape: (1737, 6502)\n",
      "Training score: 0.8892301785371473\n",
      "Test score: 0.8411053540587219\n"
     ]
    }
   ],
   "source": [
    "X = whole_df.drop(columns = 'subreddit_name')\n",
    "y = whole_df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "tvec = TfidfVectorizer(max_df=0.12, max_features=6_500, ngram_range=(1,2), stop_words = expanded_stopwords)\n",
    "vectorized_train = tvec.fit_transform(X_train['all_words'])\n",
    "vectorized_test = tvec.transform(X_test['all_words'])\n",
    "\n",
    "vectorized_train = pd.DataFrame(vectorized_train.todense(), columns = tvec.get_feature_names_out())\n",
    "vectorized_test = pd.DataFrame(vectorized_test.todense(), columns = tvec.get_feature_names_out())\n",
    "\n",
    "print('vectorized_train shape:', vectorized_train.shape)\n",
    "print('vectorized_test shape:', vectorized_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "print('X_train reset index shape:', X_train.shape)\n",
    "print('X_test reset index shape:', X_test.shape)\n",
    "\n",
    "vectorized_train = pd.concat([X_train[['index', 'no_selftext', 'body_length']], vectorized_train], axis = 1)\n",
    "vectorized_test = pd.concat([X_test[['index', 'no_selftext', 'body_length']], vectorized_test], axis = 1)\n",
    "\n",
    "print('vectorized_train concatenated shape:', vectorized_train.shape)\n",
    "print('vectorized_test concatenated shape:', vectorized_test.shape)\n",
    "\n",
    "vectorized_train = vectorized_train.set_index('index')\n",
    "vectorized_test = vectorized_test.set_index('index')\n",
    "\n",
    "print('vectorized_train corrected index shape:', vectorized_train.shape)\n",
    "print('vectorized_test corrected index shape:', vectorized_test.shape)\n",
    "\n",
    "log = LogisticRegression(max_iter=10_000, C=1.4)\n",
    "\n",
    "log.fit(vectorized_train, y_train)\n",
    "\n",
    "print('Training score:', log.score(vectorized_train, y_train))\n",
    "print('Test score:', log.score(vectorized_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87ec9a-8dfb-44ba-b8aa-a115cb17d536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
