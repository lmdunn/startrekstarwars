{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f913ea-baa9-4b11-81fc-aab2af393652",
   "metadata": {},
   "source": [
    "I based my initial selection for imports off the work we did in the NLP Practice breakfast hour challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "95150329-11b5-4600-b86d-991db9701b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e13433b-bd35-4a58-bac0-3d6b93af2509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>startrek</td>\n",
       "      <td>I’m beaming and I had to share - Sir Patrick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>startrek</td>\n",
       "      <td>America and the Star Trek Universe. Roe Vs Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>startrek</td>\n",
       "      <td>Analysis: Star Trek: The Next Generation’ Gue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>startrek</td>\n",
       "      <td>One of the first occasions in which the word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>startrek</td>\n",
       "      <td>Is A TOS Reboot Coming Soon?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_name                                          all_words\n",
       "0       startrek   I’m beaming and I had to share - Sir Patrick ...\n",
       "1       startrek   America and the Star Trek Universe. Roe Vs Wa...\n",
       "2       startrek   Analysis: Star Trek: The Next Generation’ Gue...\n",
       "3       startrek   One of the first occasions in which the word ...\n",
       "4       startrek                       Is A TOS Reboot Coming Soon?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cleaned_all_text2022-06-27.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96423d-bcd5-42bf-b7e1-266ff80f52f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Accuracy\n",
    "\n",
    "Our baseline accuracy is 57.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55197498-414c-49ce-a663-7450d7628006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "starwars    0.572228\n",
       "startrek    0.427772\n",
       "Name: subreddit_name, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit_name.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e2e74-2906-4836-8863-75b083bfa74c",
   "metadata": {},
   "source": [
    "# Building Functions to Lemmatize and Stem Text\n",
    "\n",
    "I'm using the workflow from the NLP Practice breakfast hour as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3902cbf6-8ad4-4c19-99ec-d544e39ff1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test phrase: my computer computes computationally\n",
      "Test phrase lemmatized: my computer computes computationally\n",
      "Test phrase stemmed: my comput comput comput\n",
      "\n",
      "Test phrase: studies studying cries cry\n",
      "Test phrase lemmatized: study studying cry cry\n",
      "Test phrase stemmed: studi studi cri cri\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    split_text = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_text])\n",
    "\n",
    "def stem_text(text):\n",
    "    split_text = text.split()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    return ' '.join([p_stemmer.stem(word) for word in split_text])\n",
    "\n",
    "test_phrase1 = 'my computer computes computationally'\n",
    "print(f'Test phrase: {test_phrase1}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase1)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase1)}')\n",
    "print('')\n",
    "\n",
    "test_phrase2 = 'studies studying cries cry'\n",
    "print(f'Test phrase: {test_phrase2}')\n",
    "print(f'Test phrase lemmatized: {lemmatize_text(test_phrase2)}')\n",
    "print(f'Test phrase stemmed: {stem_text(test_phrase2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4027de-85f0-40c0-98e7-f4b5642eaaf3",
   "metadata": {},
   "source": [
    "# Prepping Data for Modeling\n",
    "\n",
    "Getting training and test data set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cde5407a-1409-4f29-8a76-abcfb2bb5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3753,)\n",
      "y_train shape: (3753,)\n",
      "X_test shape: (1252,)\n",
      "y_test shape: (1252,)\n",
      "\n",
      "Dataframe shape: (5005, 2)\n",
      "\n",
      "y_train value counts: starwars    0.572342\n",
      "startrek    0.427658\n",
      "Name: subreddit_name, dtype: float64\n",
      "y_test value counts: starwars    0.571885\n",
      "startrek    0.428115\n",
      "Name: subreddit_name, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df['all_words']\n",
    "y = df['subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('')\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('')\n",
    "print('y_train value counts:', y_train.value_counts(normalize = True))\n",
    "print('y_test value counts:', y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed5a97-26e7-4b49-a17c-94d8bdab42e4",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "I built a function to streamline the pipeline/gridsearch writing.\\\n",
    "I modeled with out adding the proper name stop words first.\n",
    "\n",
    "I began by working on CountVectorizer and LogisticRegression, tweaking the parameters and running the model repeatedly. The score was very high to start, probably because of the proper names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bc657-a8f5-4ceb-b664-61c734da129b",
   "metadata": {},
   "source": [
    "## Building a Gridsearch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "996402a1-58d3-4005-bb23-beeec29dae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_grid(pipe_params, grid_params):\n",
    "    '''\n",
    "    This function is designed to streamline gridsearching.\n",
    "    It returns a gridsearch named 'gs'\n",
    "    'pipe_params' should be a list of tuples consisting of a series of name/transform pairs followed by a name/model, \n",
    "            e.g. [('cvec', CountVectorizer()), ('log', LogisticRegression())]\n",
    "    'grid_params' should be a series of parameters for those transforms and the model in the form of a dictionary,\n",
    "            e.g. {'cvec__ngram_range': [(1,1), (1,2)]}\n",
    "    Be sure the names for the 'pipe_params' and in the 'grid_params match'\n",
    "    '''\n",
    "    global gs\n",
    "    \n",
    "    pipe = Pipeline(pipe_params)\n",
    "    \n",
    "    gs = GridSearchCV(pipe, grid_params)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    return gs, print(f'Time to run function: {time.time() - t0}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe64ddd-8b8e-4fae-9e18-c89170177a71",
   "metadata": {},
   "source": [
    "## Model 1 - CountVectorizer and LogististicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d15fa-c75b-4a14-ab28-42e32f1a031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline()\n",
    "GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b3dc7d7-8f5f-4bc9-bd1c-cef93986e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1c16d53-3eb0-4dca-bdd6-4ae0e889fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87a75306-c9c4-4c7b-b133-6519eb1dd17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-05\n",
      "78.26734280586243\n",
      "Best parameters: {'cvec__max_df': 0.95, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.95, 0.98, 1.0],\n",
    "    'cvec__max_features': [None, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b33592f4-73a7-450d-aadd-d1b61919fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "24.25687074661255\n",
      "Best parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__max_features': [2_000, 3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d39bcfc9-cb02-44c5-9900-7f1ffb3aa622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.4066696166992188e-05\n",
      "10.730860710144043\n",
      "Best parameters: {'cvec__max_df': 0.85, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.85, 0.9],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f37c3ab0-3ec6-4099-ac3e-61b1c65a959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "8.003306865692139\n",
      "Best parameters: {'cvec__max_df': 0.75, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.75, 0.8, 0.85],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e538b4e-6668-4e36-bd59-d12d7d73d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-06\n",
      "8.02409815788269\n",
      "Best parameters: {'cvec__max_df': 0.5, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.5, 0.6, 0.75],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57f6c724-f875-4fa3-a17b-8405b3424e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.3113021850585938e-05\n",
      "8.132829189300537\n",
      "Best parameters: {'cvec__max_df': 0.3, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9845456967759126\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.3, 0.4, 0.5],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0394af39-cbe2-4a0e-9a46-01e8c10c370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "7.960031986236572\n",
      "Best parameters: {'cvec__max_df': 0.2, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9840127897681854\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.1, 0.2, 0.3],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4289933c-2215-4d08-92af-b1e49542edc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "8.004861831665039\n",
      "Best parameters: {'cvec__max_df': 0.2, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9840127897681854\n",
      "Test score: 0.9464856230031949\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.1, 0.15, 0.2],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb0df2d4-fc56-4be6-a2c1-91ca150eff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "8.008489847183228\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19, 0.2, 0.21],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c29ed9a-e2c4-48c6-b113-6dbf52474d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-05\n",
      "5.394302845001221\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.17, 0.18, 0.19],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50028899-a051-43cd-b0d6-7ab54e0a01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "5.5713582038879395\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "311df1da-38cb-44d1-a913-a37004169b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "5.639925003051758\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_500, 3_000, 3_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83f11f8a-47ac-4107-a734-09e84300bc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "5.585331916809082\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_900, 3_000, 3_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3608880-2e67-4f87-8c47-1d4c0b593856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "5.585331916809082\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_900, 3_000, 3_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e88c11cb-3e78-4307-a198-d01b3ed2a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "74.4193480014801\n",
      "Best parameters: {'cvec__max_df': 0.2, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__preprocessor': None}\n",
      "Training score: 0.9869437783106848\n",
      "Test score: 0.9400958466453674\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.18, 0.19, 0.2],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "840c3d74-01b8-4d2d-84f6-f26533d6eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "12.486733198165894\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [2_900, 3_000, 3_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa4ff49b-2658-429d-9eab-de39c4b6c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.3113021850585938e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1313, in fit_transform\n",
      "    self._validate_params()\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _validate_params\n",
      "    check_scalar(self.min_df, \"min_df\", numbers.Real, min_val=0.0, max_val=1.0)\n",
      "  File \"/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1307, in check_scalar\n",
      "    raise TypeError(f\"{name} must be an instance of {target_type}, not {type(x)}.\")\n",
      "TypeError: min_df must be an instance of <class 'numbers.Real'>, not <class 'NoneType'>.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/lorendunn/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.85877284 0.8124087 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8016412258148193\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__min_df': 0.05, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.8654409805488942\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__min_df': [None, 0.05, 0.1],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b03ed83-42f7-436c-a62e-1c6688287127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "5.262088775634766\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__min_df': 0.04, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.8928856914468425\n",
      "Test score: 0.8881789137380192\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__min_df': [0.04, 0.05, 0.06],\n",
    "    'cvec__max_features': [3_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dfefcc58-485a-4011-a553-321f424c8a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.198883056640625e-06\n",
      "5.461315155029297\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'log__C': 1.0}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [3_000],\n",
    "    'log__C': [0.1, 0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4192295e-c78b-4ec2-a69a-8f927fc9e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "5.550469160079956\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'log__C': 0.9}\n",
      "Training score: 0.9829469757527312\n",
      "Test score: 0.9472843450479234\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [3_000],\n",
    "    'log__C': [0.9, 1.0, 1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1c6656d-7bcc-4cfc-83e4-8c608fc098ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "5.586487054824829\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'log__C': 0.9}\n",
      "Training score: 0.9829469757527312\n",
      "Test score: 0.9472843450479234\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [3_000],\n",
    "    'log__C': [0.9, 0.95, 1.]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95195e1a-dc59-4207-863d-74758ecfd858",
   "metadata": {},
   "source": [
    "Weirdly, the above two gridsearches are saying log__C = 0.9 is the best, but I can see it's better with 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fae1b82f-e31c-4839-b383-235da1aa8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.105170965194702\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'log__C': 1.0}\n",
      "Training score: 0.9850786037836398\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "# BEST VERSION\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,2)],\n",
    "    'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19],\n",
    "    'cvec__max_features': [3_000],\n",
    "    'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676e67d-9545-4267-970e-d057c9fd8619",
   "metadata": {},
   "source": [
    "### Model 1: CountVectorizer and LogReg, Best Version\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'log__C': 1.0}\n",
    "Training score: 0.9850786037836398\n",
    "Test score: 0.9488817891373802"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679e2bd-f2a9-4cc0-b100-398eecdc0eef",
   "metadata": {},
   "source": [
    "## Model 2: CountVectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f8ea684-71f9-41a0-b476-95fbebb53c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 0.00012183189392089844\n",
      "393.21941208839417\n",
      "Best parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 1), 'cvec__preprocessor': None}\n",
      "Training score: 0.968558486544098\n",
      "Test score: 0.9600638977635783\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': ['english'],\n",
    "    'cvec__max_df': [0.19, 0.5, 0.9],\n",
    "    'cvec__max_features': [2_000, 3_000, 5_000],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa212418-3b8b-46d5-9d91-c6152ff33158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 0.0\n",
      "43.8954119682312\n",
      "Best parameters: {'cvec__max_df': 0.5, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "Training score: 0.9720223820943246\n",
      "Test score: 0.9584664536741214\n"
     ]
    }
   ],
   "source": [
    "#because best preprocessor was 'none' I'm going to check these params with stop words\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.19, 0.5, 0.9],\n",
    "    'cvec__max_features': [2_000, 3_000, 5_000],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85a796-2bf4-48b2-aec9-000ba902633b",
   "metadata": {},
   "source": [
    "The training score was improved, but the test score was lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a5634359-d326-4ab2-83df-f82aece01695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 8.106231689453125e-06\n",
      "24.8409903049469\n",
      "Best parameters: {'cvec__max_df': 0.9, 'cvec__max_features': 5000, 'cvec__ngram_range': (1, 1)}\n",
      "Training score: 0.968558486544098\n",
      "Test score: 0.9600638977635783\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.19, 0.5, 0.9],\n",
    "    'cvec__max_features': [2_000, 3_000, 5_000],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff1772ea-70d3-4bfb-80ad-a216159e7130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.09808349609375e-05\n",
      "25.130499124526978\n",
      "Best parameters: {'cvec__max_df': 0.85, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1)}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.85, 0.9, 0.95],\n",
    "    'cvec__max_features': [4_500, 5_000, 5_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb1d7c-fd44-47e5-a9db-999001ace228",
   "metadata": {},
   "source": [
    "Narrowing to: ngram_range = (1,1) since that's consistently been best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7837e9a0-ac37-453c-99a1-5b650385ffd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0013580322265625e-05\n",
      "6.702489137649536\n",
      "Best parameters: {'cvec__max_df': 0.75, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1)}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.75, 0.8, 0.85],\n",
    "    'cvec__max_features': [3_500, 4_000, 4_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "887d1556-ffac-47e7-9f6c-08cf40f6383e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "6.700904130935669\n",
      "Best parameters: {'cvec__max_df': 0.7, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1)}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.70, 0.75, 0.77],\n",
    "    'cvec__max_features': [4_400, 4_500, 4_600],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59e0ecb2-1b2e-4b66-8253-7f053ab6e7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-05\n",
      "2.349112033843994\n",
      "Best parameters: {'cvec__max_df': 0.65, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1)}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.65, 0.70, 0.71],\n",
    "    'cvec__max_features': [4_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1cfc2425-48d8-4db2-b73a-eb2cecdaf0be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.0067901611328125e-06\n",
      "2.3762898445129395\n",
      "Best parameters: {'cvec__max_df': 0.65, 'cvec__max_features': 4500}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    #'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.55, 0.60, 0.65],\n",
    "    'cvec__max_features': [4_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a65a7f5a-9b37-4634-84f2-7989529e7182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.3555970191955566\n",
      "Best parameters: {'cvec__max_df': 0.64, 'cvec__max_features': 4500}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    #'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.64, 0.65, 0.66],\n",
    "    'cvec__max_features': [4_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6804d5ad-d8b9-446b-b2a5-f47861065c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.363154888153076\n",
      "Best parameters: {'cvec__max_df': 0.64, 'cvec__max_features': 4500}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "#BEST CVEC PARAMS\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    #'cvec__ngram_range': [(1,1)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.62, 0.63, 0.64],\n",
    "    'cvec__max_features': [4_500],\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8529647-13cf-46bc-9108-fea589c4888c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-05\n",
      "8.400366067886353\n",
      "Best parameters: {'cvec__max_df': 0.64, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'nb__alpha': 1.0}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.64],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a420749c-da07-4573-bd17-e2e70f62e66a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.9141387939453125e-06\n",
      "8.448508739471436\n",
      "Best parameters: {'cvec__max_df': 0.64, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'nb__alpha': 1.0}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.9616613418530351\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'cvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'cvec__stop_words': [None, 'english'],\n",
    "    'cvec__max_df': [0.64],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'nb__alpha': [0.9, 1.0, 1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f257a-3e8a-4122-9635-8bccb71015f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Model 2, CountVectorizer and Naive Bayes\n",
    "\n",
    "**this model is superior to model 1**\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.64, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'nb__alpha': 1.0}\n",
    "Training score: 0.9672262190247801\n",
    "Test score: 0.9616613418530351"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2528811-3276-4583-96f2-90343969b563",
   "metadata": {},
   "source": [
    "# Model 3, TfidfVectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "efb8e285-7eb5-4c33-8be8-1424d74d7c22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.7220458984375e-06\n",
      "402.31639099121094\n",
      "Best parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__preprocessor': <function stem_text at 0x7f9568451d30>}\n",
      "Training score: 0.9752198241406874\n",
      "Test score: 0.9440894568690096\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.1, 0.5, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85f4f866-7e43-48b4-9a23-af0640c44cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 0.0\n",
      "45.24504804611206\n",
      "Best parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9664536741214057\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.1, 0.5, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "70263516-5011-4120-b460-86f21b04a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "45.448556900024414\n",
      "Best parameters: {'tvec__max_df': 0.4, 'tvec__max_features': 4000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9832134292565947\n",
      "Test score: 0.9640575079872205\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.4, 0.5, 0.6],\n",
    "    'tvec__max_features': [4_000, 5_000, 6_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a2fef01c-92cf-4725-bc8b-be56af9f10a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "15.6185941696167\n",
      "Best parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 4000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9832134292565947\n",
      "Test score: 0.9640575079872205\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.5],\n",
    "    'tvec__max_features': [4_000, 5_000, 6_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c97cf335-bbcf-46b8-b0ea-1419a9ced711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "15.379570960998535\n",
      "Best parameters: {'tvec__max_df': 0.4, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9664536741214057\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.4, 0.5, 0.6],\n",
    "    'tvec__max_features': [5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7c4fdf4-1d39-4236-b0d8-90d1fa5ba60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "45.27949118614197\n",
      "Best parameters: {'tvec__max_df': 0.35, 'tvec__max_features': 4500, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9656549520766773\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.35, 0.4, 0.45],\n",
    "    'tvec__max_features': [4_500, 5_000, 5_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4f34a9bf-0a4c-44b5-ba95-9e0c705db48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "4.940686941146851\n",
      "Best parameters: {'tvec__max_df': 0.35, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9664536741214057\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.35, 0.4, 0.45],\n",
    "    'tvec__max_features': [5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "286def93-db4a-43ee-84bc-0f8ef430a6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "14.061980247497559\n",
      "Best parameters: {'tvec__max_df': 0.35, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9672523961661342\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.35, 0.4, 0.45],\n",
    "    'tvec__max_features': [4_900, 5_000, 5_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "690aa773-8168-4e6f-aeb0-c3cb0dd9cf65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "13.970189094543457\n",
      "Best parameters: {'tvec__max_df': 0.35, 'tvec__max_features': 4700, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9834798827604583\n",
      "Test score: 0.9656549520766773\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.35, 0.4, 0.45],\n",
    "    'tvec__max_features': [4_700, 4_800, 4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b59bf73-304c-4448-a058-bb3b45c8177e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.0531158447265625e-06\n",
      "4.952348947525024\n",
      "Best parameters: {'tvec__max_df': 0.25, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9834798827604583\n",
      "Test score: 0.9664536741214057\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.25, 0.30, 0.35],\n",
    "    'tvec__max_features': [4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1bc86523-eddc-48e3-b6a5-2a9a9af0265f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "3.387397050857544\n",
      "Best parameters: {'tvec__max_df': 0.35, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9672523961661342\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.35, 0.40],\n",
    "    'tvec__max_features': [4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d33693d8-a9f8-4b01-ba35-110cb8289440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.0994415283203125e-06\n",
      "1.9567821025848389\n",
      "Best parameters: {'tvec__max_df': 0.4, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9672523961661342\n"
     ]
    }
   ],
   "source": [
    "# BEST FUNCTION\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.40],\n",
    "    'tvec__max_features': [4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97e7acdf-5a3f-4ca4-be55-52005fc5c422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "1.9188129901885986\n",
      "Best parameters: {'tvec__max_df': 0.4, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9837463362643218\n",
      "Test score: 0.9664536741214057\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.40],\n",
    "    'tvec__max_features': [5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e0c35701-f4d0-46a7-ac24-45248c28edb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.814697265625e-06\n",
      "4.9508209228515625\n",
      "Best parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.4, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.986144417799094\n",
      "Test score: 0.9656549520766773\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.40],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8094259-a1ca-4931-a581-e4e475ad6e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.76837158203125e-06\n",
      "4.953273057937622\n",
      "Best parameters: {'nb__alpha': 0.9, 'tvec__max_df': 0.4, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9840127897681854\n",
      "Test score: 0.9672523961661342\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.40],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.9, 1.0, 1.1]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36ba0f-b1df-4475-a5d8-0fcfe0923905",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Model 3, TfidfVectorizer and Naive Bayes\n",
    "\n",
    "Best parameters: {'tvec__max_df': 0.4, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n",
    "Training score: 0.9837463362643218\n",
    "Test score: 0.9672523961661342\n",
    "\n",
    "**slightly higher accuracy, slightly more overfit. My top pick so far because overfit is ever so slight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05bc3d-1e8a-4603-addc-b8e37a39adc6",
   "metadata": {},
   "source": [
    "# Model 4, TfidfVectorizer and LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1be2b061-4b87-4d05-914f-c0af2c3ff5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.867813110351562e-06\n",
      "405.03462195396423\n",
      "Best parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 5000, 'tvec__ngram_range': (1, 1), 'tvec__preprocessor': None}\n",
      "Training score: 0.9728217426059153\n",
      "Test score: 0.9488817891373802\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    #'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.5, 0.75, 1.0],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b888caf6-31a0-4a17-89b9-c3d0217880ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "47.735355854034424\n",
      "Best parameters: {'tvec__max_df': 0.5, 'tvec__max_features': 3000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9757527311484147\n",
      "Test score: 0.950479233226837\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.5, 0.75, 1.0],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6c3bbe86-cd9b-4998-a66d-de87ab3952af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "48.32843899726868\n",
      "Best parameters: {'tvec__max_df': 0.3, 'tvec__max_features': 3000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9757527311484147\n",
      "Test score: 0.950479233226837\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.3, 0.4, 0.5],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "996c9aa8-10e5-4e3a-be31-4c92877e7b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "47.3973445892334\n",
      "Best parameters: {'tvec__max_df': 0.2, 'tvec__max_features': 3000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9781508126831868\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__max_df': [0.1, 0.2, 0.3],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2fd007e9-2189-40e9-9168-45814606e4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.1920928955078125e-06\n",
      "6.969743251800537\n",
      "Best parameters: {'tvec__max_df': 0.25, 'tvec__max_features': 3000, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9778843591793233\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.15, 0.2, 0.25],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "86b8c5dd-3633-4321-bfe5-1144c01fd193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "6.891188144683838\n",
      "Best parameters: {'tvec__max_df': 0.24, 'tvec__max_features': 3000, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9778843591793233\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.24, 0.25, 0.26],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb8d04f9-decc-4635-8105-58a7a75d353d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "7.060287952423096\n",
      "Best parameters: {'tvec__max_df': 0.22, 'tvec__max_features': 3000, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9778843591793233\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22, 0.23, 0.24],\n",
    "    'tvec__max_features': [2_000, 3_000, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3d15e904-38a8-42a8-b6f6-514739bf6349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.3874549865722656\n",
      "Best parameters: {'tvec__max_df': 0.22, 'tvec__max_features': 3000, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9778843591793233\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22],\n",
    "    'tvec__max_features': [2_500, 3_000, 3_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "42731c56-3238-4903-97c8-98028b7e0261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "2.3951382637023926\n",
      "Best parameters: {'tvec__max_df': 0.22, 'tvec__max_features': 2900, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.977351452171596\n",
      "Test score: 0.9512779552715654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22],\n",
    "    'tvec__max_features': [2_900, 3_000, 3_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "69dbcb66-5a5c-4c30-a16e-0e44acfb242e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "2.3748159408569336\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.22, 'tvec__max_features': 2900, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9866773248068212\n",
      "Test score: 0.957667731629393\n"
     ]
    }
   ],
   "source": [
    "# BEST VERSION\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22],\n",
    "    'tvec__max_features': [2_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aed67347-709a-4a83-aa78-279b9ce4cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.413182020187378\n",
      "Best parameters: {'log__C': 2.5, 'tvec__max_df': 0.22, 'tvec__max_features': 2900, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9920063948840927\n",
      "Test score: 0.9584664536741214\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22],\n",
    "    'tvec__max_features': [2_900],\n",
    "    'log__C': [1.5, 2.0, 2.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f2027cbf-e7bd-4135-8ac6-e19c1431122d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.0994415283203125e-06\n",
      "2.4172849655151367\n",
      "Best parameters: {'log__C': 5, 'tvec__max_df': 0.22, 'tvec__max_features': 2900, 'tvec__stop_words': 'english'}\n",
      "Training score: 0.9952038369304557\n",
      "Test score: 0.9584664536741214\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    #'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    #'tvec__preprocessor': [None, lemmatize_text, stem_text],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__max_df': [0.22],\n",
    "    'tvec__max_features': [2_900],\n",
    "    'log__C': [2.5, 3.0, 5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408cd4c-b947-49c8-a866-edd762412bba",
   "metadata": {},
   "source": [
    "## Model 4, TfidfVectorizer and LogReg Best Version\n",
    "\n",
    "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.22, 'tvec__max_features': 2900, 'tvec__stop_words': 'english'}\n",
    "Training score: 0.9866773248068212\n",
    "Test score: 0.957667731629393"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08f796-73ab-4bf3-bef7-c7d43a0f77fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring Models with Proper Name Stop Words Removed\n",
    "\n",
    "I have lists of proper names I created in EDA. I'd like to see how the models perform when I make things a little harder for the model.\n",
    "\n",
    "Starting with best versions of other models and iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42d5a5b9-eb3f-4c5a-a6e8-ba3f9760e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_names = ['snw', 'discovery', 'force', 'federation', 'darth', 'borg', 'wars', 'anakin',\\\n",
    "                'lightsaber', 'vader', 'picard', 'skywalker', 'sith', 'spock', 'starfleet', 'leia',\\\n",
    "                'star', 'tng', 'ds9', 'reva', 'strange', 'luke', 'series', 'obi', 'clone', 'trek',\\\n",
    "                'enterprise', 'disney', 'wan', 'voyager', 'jedi', 'kenobi', 'tos']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(proper_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a7806423-3d2c-4715-b0a8-44acfa6d1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_proper_names = ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia',\\\n",
    "                         'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet',\\\n",
    "                         'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation',\\\n",
    "                         'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf',\\\n",
    "                         'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith',\\\n",
    "                         'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']\n",
    "\n",
    "\n",
    "expanded_stop_words = text.ENGLISH_STOP_WORDS.union(expanded_proper_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a594c52-412c-4785-9f7c-2301de306222",
   "metadata": {},
   "source": [
    "# Model 3.2, Tvec, NB, taking out top proper names and show specific references.\n",
    "Starting with best version and iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b8309549-fded-4865-9328-229c1be5b07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "44.57108020782471\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'neither', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'must', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'obi', 'everyone', 'because', 'anything', 'whether', 'series', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'somehow', 'some', 'perhaps', 'always', 'de', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'along', 'but', 'about', 'every', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'then', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'clone', 'might', 'together', 'has', 'if', 'off', 'why', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9483080202504663\n",
      "Test score: 0.8777955271565495\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [stop_words, proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7d8d7212-36a5-4823-94b2-4d4417c7796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "44.001856088638306\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9381827871036504\n",
      "Test score: 0.8586261980830671\n"
     ]
    }
   ],
   "source": [
    "#using expanded_stop_words makes it marginally weaker, but not much, really\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.1, 0.40, 0.9],\n",
    "    'tvec__max_features': [2_000, 3_000, 4_900]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d6c8909a-7db3-421d-b4a6-2de575a6b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "43.8257269859314\n",
      "Best parameters: {'tvec__max_df': 0.1, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9381827871036504\n",
      "Test score: 0.8586261980830671\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1),(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.05, 0.1, 0.2],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8494753f-df41-4876-a858-e91e7e6efa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "13.498803853988647\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.09, 0.1, 0.11],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "162e303c-40d0-4d2c-b38a-ea544bc54cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "6.10121488571167\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.07, 0.08, 0.09],\n",
    "    'tvec__max_features': [4_500, 4_900, 5_300]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "047c8595-a91a-4c96-ab56-b8bd4f0dcf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.1546919345855713\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4800, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9387156941113776\n",
      "Test score: 0.860223642172524\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "82ca2e3e-24cb-444d-abb1-8c24a8d4ca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.814697265625e-06\n",
      "2.118048906326294\n",
      "Best parameters: {'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9389821476152411\n",
      "Test score: 0.8610223642172524\n"
     ]
    }
   ],
   "source": [
    "# best without changing alpha\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900, 5_300, 5_700]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "79063945-fd16-4562-9d83-2a0432884f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "2.1531660556793213\n",
      "Best parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9536370903277378\n",
      "Test score: 0.8769968051118211\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL 3.2, EXPANDED STOP WORDS\n",
    "\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70e94e6d-1f76-4164-aca3-804f1b43b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.1075940132141113\n",
      "Best parameters: {'nb__alpha': 0.3, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9597655209166001\n",
      "Test score: 0.8761980830670927\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.09],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'nb__alpha': [0.3, 0.5, 0.7]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657230d4-6897-4d7d-bddd-d628d4b67ade",
   "metadata": {},
   "source": [
    "## Model 3.2 Best Version, Tvec, Log\n",
    "\n",
    "BBest parameters: {'nb__alpha': 0.5, 'tvec__max_df': 0.09, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': expanded_stop_words\n",
    "Training score: 0.9536370903277378\n",
    "Test score: 0.8769968051118211"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0657d-bb3c-4833-8489-971285b7bd95",
   "metadata": {},
   "source": [
    "## Model 2.2, Cvec, NB \n",
    "Working from best version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9744268b-7353-482f-b072-0a862db87555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.245208740234375e-06\n",
      "43.72111892700195\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 6500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.929656274980016\n",
      "Test score: 0.8793929712460063\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [2_500, 4_500, 6_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "583d64d5-a794-47e6-aa2d-70686852281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-06\n",
      "43.68476104736328\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9357847055688783\n",
      "Test score: 0.8793929712460063\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [5_500, 6_500, 7_500]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8bd536-6aa9-4518-886a-1c47dc700e6a",
   "metadata": {},
   "source": [
    "No improvement in test score, less strong fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3bd11487-fb90-42fe-be59-507bef4acd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.9604644775390625e-06\n",
      "43.8722620010376\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [6_000, 6_500, 7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "73b9c3e7-5add-498c-9bb5-5856318d6157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.9073486328125e-06\n",
      "43.96654176712036\n",
      "Best parameters: {'cvec__max_df': 0.34, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.34, 0.64, 0.94],\n",
    "    'cvec__max_features': [6_900, 7_000, 7_100]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6a8e2fa3-3751-4506-85da-fc41a6ccc936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.9141387939453125e-06\n",
      "14.783697843551636\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.24, 0.34, 0.44],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cc9b615e-1edf-4f77-9678-0589776205f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "2.161890983581543\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.2, 0.24, 0.28],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9ad28c60-5bd2-46c2-8cff-fcfb8b3950f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.6689300537109375e-06\n",
      "2.161890983581543\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "#BEST VERSION WITHOUT CHANGING NB PARAMETERS\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "47049cc8-b068-4acf-a399-77476ca3c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "2.142659902572632\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'nb__alpha': 1.0}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000],\n",
    "    'nb__alpha': [0.5, 1.0, 1.5]    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7aa5e82b-7836-44f5-8d40-37dc44a4ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.0531158447265625e-06\n",
      "2.1283137798309326\n",
      "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'nb__alpha': 1.0}\n",
      "Training score: 0.9344524380495604\n",
      "Test score: 0.8801916932907349\n"
     ]
    }
   ],
   "source": [
    "#BEST ALPHA IS 1.0\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('nb', MultinomialNB())]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.24],\n",
    "    'cvec__max_features': [7_000],\n",
    "    'nb__alpha': [0.9, 1.0, 1.1]    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ada30-acd5-4e62-918d-60072599c62d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Version of Model 2.2, Cvec & NB\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.24, 'cvec__max_features': 7000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stop_words, 'nb__alpha': 1.0}\n",
    "Training score: 0.9344524380495604\n",
    "Test score: 0.8801916932907349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a325f0-4dee-46e8-9195-2e7399e534c6",
   "metadata": {},
   "source": [
    "## Model 1.2, Cvec & LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "06bbf1a4-07de-43f3-85ad-2e75dc600907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.2159347534179688e-05\n",
      "56.74627709388733\n",
      "Best parameters: {'cvec__max_df': 0.19, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9669597655209166\n",
      "Test score: 0.8410543130990416\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [1_500, 3_000, 4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dd526149-2141-46af-be44-e22ac33a1a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 3.814697265625e-06\n",
      "59.06657409667969\n",
      "Best parameters: {'cvec__max_df': 0.37, 'cvec__max_features': 4000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.968558486544098\n",
      "Test score: 0.8306709265175719\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.01, 0.19, 0.37],\n",
    "    'cvec__max_features': [4_000, 4_500, 5_000],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "296c55b7-3f7d-4de1-8695-10222ffa8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-06\n",
      "20.858755111694336\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.84185303514377\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1),(1,2)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.09, 0.19, 0.29],\n",
    "    'cvec__max_features': [4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c616f513-106c-4a14-9b85-987a95327288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "6.810006856918335\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.84185303514377\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'cvec__max_df': [0.04, 0.09, 0.14],\n",
    "    'cvec__max_features': [4_500],\n",
    "    #'log__C': [1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "88c64436-1265-4517-a345-4aa90b36264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.4066696166992188e-05\n",
      "3.3841030597686768\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.5}\n",
      "Training score: 0.9746869171329603\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f754f696-6d9c-45bf-8770-589823ca2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 7.152557373046875e-07\n",
      "3.591322183609009\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 2.5}\n",
      "Training score: 0.9832134292565947\n",
      "Test score: 0.8434504792332268\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.5, 2.0, 2.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5e19fa96-aa30-4f60-9925-f28c3f596eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.9604644775390625e-06\n",
      "3.9458119869232178\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.7}\n",
      "Training score: 0.9770849986677325\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.3, 1.5, 1.7]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f1e0bc1a-0f9b-42fd-bb9e-5b706dc1c0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 4.76837158203125e-06\n",
      "2.444786787033081\n",
      "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'}), 'log__C': 1.5}\n",
      "Training score: 0.9746869171329603\n",
      "Test score: 0.8442492012779552\n"
     ]
    }
   ],
   "source": [
    "#best version of 1.2\n",
    "pipe_params = [('cvec', CountVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter = 10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'cvec__stop_words': [expanded_stop_words],\n",
    "    'cvec__max_df': [0.09],\n",
    "    'cvec__max_features': [4_500],\n",
    "    'log__C': [1.5, 1.6]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a414-f446-460c-b2b2-0af6be9baf80",
   "metadata": {},
   "source": [
    "## Best 1.2 Cvec and LogReg\n",
    "\n",
    "Best parameters: {'cvec__max_df': 0.09, 'cvec__max_features': 4500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': expanded_stop_words, 'log__C': 1.5}\n",
    "Training score: 0.9746869171329603\n",
    "Test score: 0.8442492012779552"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412cf714-9204-46fa-898b-98cee90648ae",
   "metadata": {},
   "source": [
    "## Model 4.2 tvec, LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d951a3ee-6341-4d4b-8ec0-a06f487f5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "47.06992697715759\n",
      "Best parameters: {'tvec__max_df': 0.22, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.02, 0.22, 0.42],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dd698831-78d7-4c85-864d-e310b1e99294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 6.198883056640625e-06\n",
      "47.04106307029724\n",
      "Best parameters: {'tvec__max_df': 0.29, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': ['seven', 'clone', 'warp', 'borg', 'trilogy', 'contact', 'prequels', 'anakin', 'paramount', 'leia', 'kirk', 'wan', 'jedi', 'kenobi', 'snw', 'wars', 'vader', 'order', 'skywalker', 'klingon', 'starfleet', 'ds9', 'captain', 'maul', 'luke', 'obi', 'rebels', 'data', 'voyager', 'st', 'discovery', 'federation', 'pike', 'picard', 'mandalorian', 'klingons', 'star', 'tng', 'reva', 'strange', 'disney', 'worf', 'riker', 'empire', 'jurati', 'palpatine', 'yoda', 'force', 'darth', 'republic', 'lightsaber', 'sith', 'spock', 'boba', 'fett', 'thought', 'inquisitor', 'trek', 'enterprise', 'tos']}\n",
      "Training score: 0.9205968558486544\n",
      "Test score: 0.8338658146964856\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.15, 0.22, 0.29],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9e2892d4-96c5-465e-bf26-9a385d327951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "46.90589618682861\n",
      "Best parameters: {'tvec__max_df': 0.2, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9307220889954703\n",
      "Test score: 0.8506389776357828\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.20, 0.22, 0.24],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9dab96f4-df0a-4e0b-b68e-08843993a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "47.65503001213074\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21, 0.22, 0.23],\n",
    "    'tvec__max_features': [900, 2_900, 4_900],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "469add58-63be-4811-8bd9-5e7a70520300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "16.321935176849365\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [3_400, 4_900, 5_400],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b063b550-d981-4816-96b1-dfe94f228c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 2.1457672119140625e-06\n",
      "16.49087882041931\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_400, 4_900, 5_300],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "204fd3ed-86bd-4312-8adb-01dea48849bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 1.0967254638671875e-05\n",
      "36.74275779724121\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'tvec__stop_words': [expanded_stop_words, expanded_proper_names],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_400, 4_900, 5_300],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "887ed111-e97c-496d-b3c9-f53b3c4bd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 5.0067901611328125e-06\n",
      "5.021714925765991\n",
      "Best parameters: {'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9309885424993338\n",
      "Test score: 0.8522364217252396\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_800, 4_900, 5_000],\n",
    "    #'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0afa978e-d71b-4ed2-9bab-dc9cb21fc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.5367431640625e-07\n",
      "4.9872589111328125\n",
      "Best parameters: {'log__C': 1.5, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9509725552891021\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [0.5, 1.0, 1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "578918f3-8024-4e07-b758-b0ee8eb855ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 8.106231689453125e-06\n",
      "5.038173198699951\n",
      "Best parameters: {'log__C': 2.5, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9672262190247801\n",
      "Test score: 0.8698083067092651\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.5, 2.0, 2.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "57de9fdd-3103-4545-9246-264ab68f2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.059906005859375e-06\n",
      "5.052214860916138\n",
      "Best parameters: {'log__C': 1.6, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9547029043431922\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.4, 1.5, 1.6]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "81af6687-a446-4ac5-bc2b-8314beec79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run function: 9.059906005859375e-06\n",
      "5.052214860916138\n",
      "Best parameters: {'log__C': 1.6, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': frozenset({'trek', 'what', 'spock', 'with', 'alone', 'republic', 'order', 'call', 'mill', 'twenty', 'anyway', 'eleven', 'seven', 'upon', 'beside', 'therein', 'across', 'ltd', 'well', 'many', 'can', 'cannot', 'be', 'empire', 'neither', 'klingon', 'do', 'now', 'serious', 'behind', 'none', 'thick', 'done', 'elsewhere', 'one', 'nine', 'jurati', 'must', 'inquisitor', 'as', 'into', 'whither', 'leia', 'whereafter', 'anyhow', 'in', 'force', 'we', 'are', 'than', 'wherever', 'very', 'your', 'too', 'besides', 'contact', 'obi', 'everyone', 'because', 'anything', 'whether', 'enough', 'him', 'against', 'whatever', 'fifty', 'meanwhile', 'discovery', 'she', 'how', 'before', 'yours', 'klingons', 'boba', 'still', 'although', 'nobody', 'through', 'least', 'made', 'top', 'own', 'would', 'whoever', 'this', 'being', 'our', 'voyager', 'other', 'sincere', 'thereafter', 'hereafter', 'con', 'take', 'four', 'here', 'find', 'per', 'jedi', 'each', 'lightsaber', 'all', 'part', 'herself', 'kirk', 'back', 'everywhere', 'whole', 'themselves', 'where', 'first', 'mandalorian', 'whom', 'except', 'anakin', 'never', 'most', 'onto', 'picard', 'hereby', 'mine', 'ours', 'co', 'either', 'star', 'nor', 'much', 'another', 'yoda', 'somehow', 'some', 'perhaps', 'always', 'de', 'data', 'by', 'on', 'vader', 'tos', 'twelve', 'beforehand', 'become', 'had', 'already', 'seeming', 'afterwards', 'seems', 'me', 'out', 'could', 'yourselves', 'thru', 'bottom', 'whereupon', 'ds9', 'therefore', 'from', 'formerly', 'his', 'interest', 'whence', 'even', 'should', 'becomes', 'am', 'you', 'those', 'riker', 'among', 'wan', 'any', 'few', 'further', 'starfleet', 'luke', 'couldnt', 'latter', 'moreover', 'sith', 'warp', 'along', 'but', 'about', 'every', 'pike', 'several', 'last', 'that', 'hereupon', 'borg', 'have', 'or', 'snw', 'whenever', 'not', 'please', 'full', 'until', 'an', 'thought', 'fill', 'he', 'put', 'skywalker', 'namely', 'sometime', 'is', 'these', 'hence', 'beyond', 'hasnt', 'no', 'someone', 'while', 'something', 'due', 'disney', 'herein', 'detail', 'thin', 'empty', 'toward', 'maul', 'strange', 'latterly', 'below', 'describe', 'go', 'since', 'again', 'its', 'kenobi', 'the', 'hers', 'above', 'wars', 'others', 'found', 'paramount', 'noone', 'inc', 'see', 'i', 'down', 'they', 'himself', 'became', 'it', 'though', 'whereas', 'within', 'often', 'amoungst', 'there', 'former', 'during', 'ten', 'trilogy', 'then', 'prequels', 'more', 'fire', 'myself', 'also', 'bill', 'rather', 'becoming', 'whereby', 'my', 'when', 'up', 'and', 'via', 'once', 'seem', 'only', 'worf', 'un', 'amount', 'towards', 'amongst', 'side', 'anyone', 'sometimes', 'next', 'nowhere', 'over', 'mostly', 'eight', 'her', 'yourself', 'everything', 'name', 'whose', 'tng', 'system', 'thereupon', 'front', 'somewhere', 'both', 'between', 'fifteen', 'them', 'thus', 'five', 'reva', 'darth', 'otherwise', 'will', 'less', 'enterprise', 'third', 'without', 'around', 'forty', 'may', 'nothing', 'a', 'fett', 'six', 'two', 'ie', 'same', 'was', 'cant', 'ourselves', 'rebels', 'clone', 'might', 'st', 'together', 'has', 'if', 'off', 'why', 'captain', 'such', 'give', 'after', 'indeed', 'nevertheless', 'wherein', 'almost', 'federation', 'to', 'were', 'of', 'get', 'their', 'at', 'so', 'seemed', 'anywhere', 're', 'been', 'cry', 'thence', 'us', 'eg', 'under', 'ever', 'else', 'keep', 'hundred', 'throughout', 'move', 'yet', 'who', 'sixty', 'etc', 'palpatine', 'three', 'however', 'thereby', 'itself', 'which', 'for', 'show'})}\n",
      "Training score: 0.9547029043431922\n",
      "Test score: 0.8618210862619808\n"
     ]
    }
   ],
   "source": [
    "#PREFERRED VERSION OF THIS MODEL, 4.2\n",
    "pipe_params = [('tvec', TfidfVectorizer()), \n",
    "               ('log', LogisticRegression(max_iter=10_000))]\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__ngram_range': [(1,2)],\n",
    "    'tvec__stop_words': [expanded_stop_words],\n",
    "    'tvec__max_df': [0.21],\n",
    "    'tvec__max_features': [4_900],\n",
    "    'log__C': [1.5]\n",
    "    \n",
    "}\n",
    "\n",
    "pipe_grid(pipe_params, grid_params)\n",
    "\n",
    "t0 = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "print(time.time()-t0)\n",
    "\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'Training score: {gs.score(X_train, y_train)}')\n",
    "print(f'Test score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e201643-0181-4e64-b53f-d2319efc5dcf",
   "metadata": {},
   "source": [
    "## Best Version of 4.2 Best Log Reg for Inference\n",
    "Best parameters: {'log__C': 1.6, 'tvec__max_df': 0.21, 'tvec__max_features': 4900, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': expanded_stop_words\n",
    "Training score: 0.9547029043431922\n",
    "Test score: 0.8618210862619808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee146a67-73e6-4e46-8478-e7e3d77e966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
